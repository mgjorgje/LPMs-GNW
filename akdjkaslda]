[33mcommit 22e548c3db4bf9883aac76489d655dacc313f7d0[m[33m ([m[1;36mHEAD -> [m[1;32mmain[m[33m, [m[1;31morigin/main[m[33m, [m[1;31morigin/HEAD[m[33m)[m
Author: Simon Barthelm√© <simon.barthelme@gipsa-lab.fr>
Date:   Fri Aug 26 17:51:16 2022 +0200

    Added some feedback

[1mdiff --git a/LSLN_f_3_gaussian.png b/LSLN_f_3_gaussian.png[m
[1mnew file mode 100644[m
[1mindex 0000000..4207ff0[m
Binary files /dev/null and b/LSLN_f_3_gaussian.png differ
[1mdiff --git a/MSMN_f_3_gaussian.png b/MSMN_f_3_gaussian.png[m
[1mnew file mode 100644[m
[1mindex 0000000..26516e7[m
Binary files /dev/null and b/MSMN_f_3_gaussian.png differ
[1mdiff --git a/SSLAN_f_3_gaussian.png b/SSLAN_f_3_gaussian.png[m
[1mnew file mode 100644[m
[1mindex 0000000..1c0c887[m
Binary files /dev/null and b/SSLAN_f_3_gaussian.png differ
[1mdiff --git a/bias_variance_tradeof_01.png b/bias_variance_tradeof_01.png[m
[1mnew file mode 100644[m
[1mindex 0000000..f43083f[m
Binary files /dev/null and b/bias_variance_tradeof_01.png differ
[1mdiff --git a/low_bias_001.png b/low_bias_001.png[m
[1mnew file mode 100644[m
[1mindex 0000000..b952782[m
Binary files /dev/null and b/low_bias_001.png differ
[1mdiff --git a/low_variance_06.png b/low_variance_06.png[m
[1mnew file mode 100644[m
[1mindex 0000000..1c78a46[m
Binary files /dev/null and b/low_variance_06.png differ
[1mdiff --git a/main.tex b/main.tex[m
[1mindex 77aca25..a3db9bd 100644[m
[1m--- a/main.tex[m
[1m+++ b/main.tex[m
[36m@@ -19,13 +19,14 @@[m
 \usepackage[colorlinks=true, allcolors=blue]{hyperref}[m
 \usepackage[style=alphabetic-verb]{biblatex}[m
 \addbibresource{references.bib}[m
[31m-[m
[32m+[m[32m\usepackage{xcolor}[m
 \newtheorem{definition}{Definition}[m
 \newtheorem{theorem}{Theorem}[section][m
 \newtheorem{lemma}[theorem]{Lemma}[m
 \newtheorem{corollary}[theorem]{Corollary}[m
 \newtheorem{remark}[theorem]{Remark}[m
 [m
[32m+[m[32m\newcommand\SB[1]{\textcolor{green}{#1}}[m
 [m
 [m
 [m
[36m@@ -35,12 +36,11 @@[m
 [m
 [m
 [m
[31m-\title{Graphical Nadaraya Watson estimator}[m
[32m+[m[32m\title{Averaging on graphs: When does it work?}[m
 \author{Martin Gjorgjevski}[m
 \date{May 2022}[m
 \DeclareMathOperator\supp{supp}[m
  \DeclareMathOperator{\LPM}{LPM}[m
[31m- \DeclareMathOperator\dummy{dummy}[m
 [m
 \usepackage{microtype}[m
 [m
[36m@@ -48,9 +48,9 @@[m
 [m
 \maketitle[m
 \begin{abstract}[m
[31m-The topic of this report is a mixture of nonparametric staitstics and random graph theory.[m
[31m-We analyze a simple and intuitive estimator in a graph regression setting. Given observations associated to a subset of the nodes, the estimator simply averages the observations over the neighbours of the node in question. We consider the latent position random graph model where each node $i$ is associated to an unobserved random point $X_i$, these points being i.i.d. variables which take values in $\mathbb{R}^d$. Edges  occur independently (conditionally on the latent positions), with the probability  that nodes  $i$ and $j$ are connected by an edge equal to $k(X_i,X_j)$ where $k$ is a kernel on $\mathbb{R}^d$. While such an assumption on the data generating process may be an oversimplification for practical applications, it is a useful playground for a theoretical  understanding quantities such as sample complexities and generalization bounds.[m
[31m-Due to the connection with the classical Nadaraya Watson kernel based estimator, we call the proposed estimator the Graphical Nadaraya Watson estimator, denoted by $\hat{f}_{GNW}(x)$. In the nonparametric estimation literature it is well known that convolutional kernels need bandwith adaptation in order to achieve optimal performance, while in the graph learning literature it is well known that large social networks observed in practice are sparse (in terms of the adjacency matrix). In our setting we show that under certain conditions on $k$, these two phenomena are closely related. We show that for bounded functions $f$, $Var(\hat{f}_{GNW}(x))=\Theta(1/\deg(x))$ and for bounded and sufficiently smooth functions $f$ the mean square error of $E(\hat{f}_{GNW}(x)-f(x))^2=O(\max(1/deg(x),deg(x)/n))$. As a consequence we conclude that the Graphical Nadaraya Watson estimator is consistent for arbitrarily sparse graphs. On the other hand, $\hat{f}_{GNW}$ suffers from the problems common in statstics such as bias-variance trade-off and the curse of dimensionality. Despite the simplicity of $\hat{f}_{GNW}$, we believe that these results will contribute to improve the theoretical understanding of more sophisticated graph learning architectures  such as Graph Neural Networks (GNNs). [m
[32m+[m[32mThe content of this report is at the intersection of statistics and random graph theory.[m
[32m+[m[32mGiven a graph with a subset of labeled nodes, we are interested in the quality of the averaging estimator which for an unlabeled node predicts the average of the observations of its labeled neighbours. In order to describe the graph generating process we assume that nodes are associated to unobserved positions in $\mathbb{R}^d$ and that edges between nodes appear proportionally to the similarity of their associated positions as measured by a similarity kernel $k$. We rigorously study concentration properties, bias and variance bounds and[m
[32m+[m[32mrisk bounds under this assumption. While the estimator itself is very simple and the data generating process is too idealistic for practical applications, we believe that it is a useful playground for a theoretical understanding of quantities such as sample complexities and generalization bounds.[m
 \end{abstract}[m
 [m
 [m
[36m@@ -61,84 +61,187 @@[m [mDue to the connection with the classical Nadaraya Watson kernel based estimator,[m
 [m
 [m
 \section{Introduction}[m
[31m-\subsection{Nonparametric regression and the Nadaraya-Watson estimator}[m
[31m-[m
[31m-[m
[31m-\paragraph{Nonparametric regression} In the classical nonparametric regression problem  we are given data points  $X_1,...,X_n\in\mathbb{R}^d$ which are either fixed\footnote{For example equally spaced points on the unit cube $[0,1]^d$} or independent samples with common density $p$ and  [m
[31m-noisy observations $Y_i=f(X_i)+\epsilon_i$. Here,  $f\colon\mathbb{R}^d\to\mathbb{R}$ is an unknown function and in some suitable function class   $\mathcal{F}$ and[m
[31m-$\epsilon_1,...,\epsilon_n$ are assumed to be i.i.d. centered variables with variance $\sigma^2$. The goal is to estimate $f$. The term \textit{nonparametric} stems from the fact that the function class $\mathcal{F}$ can not be parametrized by a  subset of $\mathbb{R}^m$ for any $m\in \mathbb{N}$.[m
[31m-[m
[31m-\paragraph{The Nadaraya Watson estimator} A kernel $k$ on $\mathbb{R}^d$ is a symmetric function $k:\mathbb{R}^d\times\mathbb{R}^d\rightarrow \mathbb{R}$. The kernel $k$ is said to be positive semi definite if for any $x_1,...,x_n\in\mathbb{R}^d$, and the $n\times n$ matrix with $(i,j)$-th entry $k(x_i,x_j)$ is positive semi definite. If there exists a function $K\colon\mathbb{R}^d\to\mathbb{R}$ such that for all $x,y\in\mathbb{R}^d$, $k(x,y)=K(x-y)$ then $k$ is said to be a stationary kernel. If, in addition, for all $x,y\in\mathbb{R}^d$, $k(x,y)=K(||x-y||)$ then $k$ is said to be a radial basis kernel. The idea behind the Nadaraya Watson estimator is to \textit{choose} a stationary kernel $k$ and a bandwith parameter $h>0$ and to estimate $f$ by[m
[31m-[m
[31m-\begin{equation}[m
[31m-\label{NW_def}[m
[31m-    \hat{f}_{NW}(x)=\begin{cases}[m
[31m-    \frac{\sum_{i=1}^nY_ik(\frac{x-X_i}{h})}{\sum_{i=1}^n{k(\frac{x-X_i}{h}})} \quad &\text{if}\, \sum_{i=1}^n [m
[31m-    k(\frac{x-X_i}{h})\neq 0\\[m
[31m-    0 \quad &\text{otherwise}\\[m
[31m-    \end{cases}[m
[31m-\end{equation}[m
[31m-The quality of the estimator in terms of the $L^2$-risk $E(\hat{f}_{NW}(x)-f(x))^2$ at a fixed point $x\in\mathbb{R}^d$ will depend on the many factors, such as the regularity of $f$, the choice of the kernel $k$ but most importantly on the choice of bandwith $h>0$. The $L^2$-risk admits the bias-variance decomposition[m
[31m-[m
[31m-\begin{equation}[m
[31m-\label{eqn:bias-variance-decomp}[m
[31m-E(\hat{f}_{NW}(x)-f(x))^2=Var(\hat{f}_{NW}(x))+(E\hat{f}_{NW(x)}-f(x))^2[m
[31m-\end{equation}[m
[31m-The quantity $E(\hat{f}_{NW}(x)-f(x))$ is known as bias and under regularity assumptions on $f$ such as $\alpha$-Holder continuity, this term is upper bounded by $Ch^{\alpha}$ where $C$ depends on $k$ and $\alpha$ but not on the sample size $n$. We will give a more thorough discussion of the bias in Chapter 4. On the other hand, the variance $Var(\hat{f}_{NW}(x)$ tends to increase as $h$ decreases, this is the famous bias-variance tradeoff phenomenon. In the fixed design setting optimal rates for $h$ are available, and they are dependent on the sample size $n$ (see \cite{Tsybakov} section 1.5 and section 1.6, Proposition 1.13). The main takeaway is that the statistician should adapt $h$ with respect to the sample size $n$. [m
[31m-[m
[31m-\paragraph{Linear estimators}[m
[31m-A general class of estimators have been extensively studied in the literature both theoretically and empirically is the class of linear estimators. A linear nonparametric regression estimator for $f$ is an estimator $\hat{f}$ which can be expressed as $\hat{f}(x)=\sum_{i=1}^nY_iW_{n,i}(x)$ where $W_{n,i}(x)$ depends on $x,X_1,...,X_n$ but not on the observations $Y_1,...,Y_n$. For this class of estimators one typically takes $\mathcal{F}$ to be a Holder or a Sobolev space over a region $G\subseteq \mathbb{R}^d$. One generalization of the Nadaraya Watson estimator when $d=1$ are the local polynomial estimators which aim to estimate not only $f$ but also several of its derivatives $f^{(1)},...,f^{(l)}$. Another popular type of linear estimators are the projection estimators. They assume that $f$ belongs in a the span\footnote{potentially closed linear span} of a certain basis and then  try to estimate coefficients with respect to said basis (e.g. trigonometric basis, wavelets and  splines among others). [m
[31m-\subsection{Random graphs and Latent Position Models}[m
 [m
[32m+[m[32m\subsection{Regression on graphs}[m
[32m+[m[32m\SB{I don't think this paragraph is about regression per se, it's about[m
[32m+[m[32m  predicting graph signals}[m
[32m+[m[32mConsider a social network (i.e. a graph) where people are represented as nodes[m
[32m+[m[32mand relations between people are represented by edges, in which numerical data[m
[32m+[m[32mof certain type is available only for some people in the network. How can the[m
[32m+[m[32mmissing data be estimated? For example, \textit{LinkedIn} is a social network[m
[32m+[m[32mwhere people look for work and connections are often formed between people in[m
[32m+[m[32mthe same line of work.[m
[32m+[m[32m\SB{It's not immediately clear in this example what the missing data is; I'd[m
[32m+[m[32m  explain first what a ``graph signal'' is, that we expect them to be ``smooth''[m
[32m+[m[32mover the graph as in your example, meaning that signal values don't differ too[m
[32m+[m[32mmuch between neighbours, and finally that we can use this for prediction. This[m
[32m+[m[32mgives you a natural way of introducing your estimator: average signal value over[m
[32m+[m[32mneighbours.[m
[32m+[m[32mExplain that this simple strategy hasn't really been studied.[m[41m [m
[32m+[m[32m}[m
[32m+[m[32mIt is reasonable to expect that a candidate for a role should get the average of the salaries of his \textit{LinkedIn} connections. In reality the salary for a role depends on many factors such as level of education, previous work experience, geographical region and many other factors. However, it is reasonable to believe that connections formed on \textit{LinkedIn} have at least some similarities between these variables. In other words a candidate should not settle for a salary which is significantly lower than those of his coworkers, nor should he be able to get a salary which is significantly higher. This report is concerned with a theoretical analysis of  signal averaging on graphs which have latent[m[41m [m
[32m+[m[32mgeometric features.[m
[32m+[m
[32m+[m[32m\subsection{Averaging on graphs}[m
[32m+[m[32m\SB{Not clear to the reader what exactly you're talking about here. What[m
[32m+[m[32m  averaging? Are you talking about any technique that does prediction using some[m
[32m+[m[32m  form of  smoothing? }[m
[32m+[m[32mWhile it is intuitively clear that signal averaging on graphs in the context of[m
[32m+[m[32msemi supervised learning \SB{SSL is sort of related but not the same thing} should work to at least some extent, it is important to acknowledge that such an attempt on estimation has firm limitations. It is by no means the state of the art method in the machine learning community. In fact, it is the simplest estimator in a graph regression context that one could think of. Nevertheless, signal averaging is used in many sophisticated algorithms such as Graph Neural Networks. T[m
[32m+[m
[32m+[m
[32m+[m[32mTo our knowledge there is no theoretical study on the limitations[m
[32m+[m[32m\SB{performance, rather than limitations, you're not just studying limitations[m
[32m+[m[32m}of this method in the machine learning literature. This is due to the lack of[m
[32m+[m[32mstatistical modeling in the graph structure \SB{Is it? How do you know?}. We[m
[32m+[m[32mneed to make assumptions about how such graphs are generated. In this report we[m
[32m+[m[32mwill adopt a popular framework for analyzing graph averaging known as the Latent[m
[32m+[m[32mPosition Model. With this goal in mind, we give some background on Random Graph[m
[32m+[m[32mModels used in statistics.[m
[32m+[m[32m\SB{The relevant distinction to make is between analyses that consider a fixed,[m
[32m+[m[32m  given graph and look at the performance of a certain method for different[m
[32m+[m[32m  classes of signals, and analyses that treat the graph as random. Point out[m
[32m+[m[32m  that you are doing the latter, and have very weak assumptions on the signals.  }[m
[32m+[m[32m\subsection{Random graphs}[m
[32m+[m[32m\SB{You need a paragraph at the beginning of this section that defines random[m
[32m+[m[32m  graphs, and explains which random graphs you are interested in and why}[m
 \paragraph{The Erdos-Renyi Model}[m
 [m
[31m-The most well known random graph model is the Erdos-Renyi random graph $G(n,p)$, for positive integer $n$ and $0\leq p\leq 1$. This model samples a random graph on $n$ vertices with edges between vertices appearing independently with probability $p$. Results about $G(n,p)$ are stated  asymptotically, that is as $n\to\infty$, for a certain range of values of $p$ (more often than not depending on $n$), a graph sampled from $G(n,p)$ has a certain property with overwhelming probability. In their pioneering work, Erdos and Renyi show that for many graphical properties there is a sharp treshold $p_c$ in the sense that for $p>p_c$ almost all $G(n,p)$ graphs have the property, while for $p<p_c$ almost none of them have the property. Classical examples are $p_c=1/n$ for the emergence of the giant component and $p_c=\log(n)/n$ for connectedness. [m
[31m-[m
[32m+[m[32mThe most well-known random graph model is the Erdos-Renyi random graph $G(n,p)$ with parameters $n\in\mathbb{N}$ and $0\leq p\leq 1$. This model samples a random graph on $n$ nodes with edges between nodes appearing independently with probability $p$. In other words, any two individuals have the same probability $p$ to form a connection, and connections between individuals are independent. This model is usually studied in the limit as $n\to\infty$ because of the concentration of measure phenomena. In their pioneering work Erdos and Renyi show that for many graphical properties such as connectedness or size of components a graph sampled from $G(n,p)$ has certain properties with overwhelming probability. In particular, there is a sharp threshold $p_c$ in the sense that for $p>p_c$ almost all $G(n,p)$ graphs have the property, while for $p<p_c$ almost none of them do. Classical examples are $p_c=1/n$ for the emergence of the giant component and $p_c=\log(n)/n$ for connectedness.[m[41m [m
[32m+[m[32m\SB{Here would be a good place to point out that real world graphs do not have a[m
[32m+[m[32mgiant component}[m
 [m
 \paragraph{Large Graphs from Real World Data}[m
 [m
[31m-Many large complex networks in the real world such as the World Wide Web, Movie actor collaboration networks, Citation Networks to name a few, have properties which are not present in the Erdos-Renyi graph. When such networks were compared to Erdos Renyi Random graph with same number of nodes and same average degree, it is observed that cliques in the Real World Networks form more often than in their Erdos-Renyi have  counterpart, Similarly, Real World Networks have degree distributions which typically obey a power law, i.e. the proportion of vertices which have degree $k$ is of the order $k^{-\alpha}$, while for their Erdos-Renyi model counterpart this should be a Poisson distribution. Such observations prompted the scientific community to consider different models for the data generating process which can better explain these phenomena (\cite{Albert}, Section 2).  [m
[31m-[m
[31m-\paragraph{The Stochastic Block Model} The Stochastic Block Model of Holland and Leindhart (\cite{Holland1983StochasticBF}) naturally includes clustering of vertices. It assumes that $n$ vertices are randomly sampled from $K$[m
[31m-communities, and that conditionally on these communities edges form independently with probability depending only on the communities. More formally, $SBM(n,p,W)$ where $n$ is a positive integer, $p=(p_1,...,p_K)$ is $K$ dimensional vector with $0\leq p_l\leq 1$ and $\sum_{l=1}^Kp_l=1$ and $W$ is a $K\times K$ symmetric matrix with entries $0\leq w_{i,j}\leq 1$ generates edges on vertices $[n]$ by first randomly assigning a community $C_1,...,C_K$ to each node with $P(i\in C_l)=p_l$ (these assignments are independent over distinct vertices) and then generating edges depending on the community of the endpoints of the edge, that is $P(i\sim j| i\in C_l,j\in C_s)=w_{ls}$. There are several questions in the $SBM$ model such as deciding if an observed graph is indeed sampled by an $SBM$, under which conditions is there a way to fully or partially recover communities based on a single observed graph. The Stochastic Block Model has a long history in the statistics literature (  \cite{Snijders}). For recent developments we refer to (\cite{Abbe}).[m
[31m-[m
[31m-\paragraph{The Random Geometric Graph} Another popular random graph model is the Geometric Random Graph, which is generated by sampling $n$ independent random variables $X_1,...,X_n\in\mathbb{R}^d$ and placing edges between nodes if the sampled points are sufficiently close, that is there is $h>0$ such that $P(i\sim j)=I(||X_i-X_j||\leq h)$. The value $h$ controls how well connected the graph is, in the sense that smaller values of $h$ give rise to sparser graphs. It is common to study how the behavior of certain graph statistics (such as degree distributions, average degrees and  subgraph counts) changes with respect to $h$. A classical treatment of this topic is the Monograph of Penrose (\cite{Penrose2003RandomGG})[m
[32m+[m[32mMany large complex networks in the real world such as the World Wide Web, Movie[m
[32m+[m[32mactor collaboration networks, Citation Networks to name a few, have properties[m
[32m+[m[32mwhich are not present in the Erdos Renyi Random graph model. That is to say,[m
[32m+[m[32mconnections are not generated independently with equal probability. \SB{The[m
[32m+[m[32m  previous two sentences are not equivalent, you can't link them with ``that is[m
[32m+[m[32m  to say''} When such networks were compared[m
[32m+[m[32mto Erdos-Renyi Random graph with same number of nodes and same average degree,[m
[32m+[m[32mit was observed that cliques in the Real World Networks form more often than in[m
[32m+[m[32mtheir Erdos-Renyi have counterpart. Similarly, Real World Networks have degree[m
[32m+[m[32mdistributions which typically obey a power law, i.e. the proportion of vertices[m
[32m+[m[32mwhich have degree $k$ is of the order $k^{-\alpha}$, while for their Erdos-Renyi[m
[32m+[m[32mmodel counterpart the same statistic should resemble a Poisson distribution.[m
[32m+[m[32mSuch observations prompted the scientific community to consider different models[m
[32m+[m[32mfor the data generating process which can better explain these phenomena[m
[32m+[m[32m(\cite{Albert}, Section 2). Among the vast literature on this topic, we focus[m
[32m+[m[32mour attention on two examples: the Stochastic Block Model and the Random[m
[32m+[m[32mGeometric Graph.[m
[32m+[m
[32m+[m[32m\paragraph{The Stochastic Block Model} The Stochastic Block Model was introduced by Holland and Leindhart (\cite{Holland1983StochasticBF}). It assumes that the nodes of the observed graph naturally belong to communities and that the probability of edge between two vertices depends only on their communities. The main tasks for the Stochastic block model framework is community detection i.e. clustering vertices within their communities.[m
[32m+[m
[32m+[m[32m\paragraph{The Random Geometric Graph} Another popular random graph model is the Geometric Random Graph, which is generated by sampling $n$ independent random variables $X_1,...,X_n\in\mathbb{R}^d$ and placing edges between nodes $i$ and $j$ if the sampled points $X_i$ and $X_j$ are sufficiently close, i.e. there is some $h>0$ such that $P(i\sim j)=I(||X_i-X_j||\leq h)$. The value $h$ controls how well connected the graph is, in the sense that smaller values of $h$ give rise to sparser graphs. It is common to study how the behavior of certain graph statistics (such as empirical degree distributions, empirical average degrees and  empirical subgraph counts) changes with respect to $h$. A classical treatment of this topic is the Monograph of Penrose (\cite{Penrose2003RandomGG})[m
 .[m
 [m
 \begin{figure}[m
     \centering[m
[31m-    \includegraphics[width=0.75\textwidth]{sparse_rgg.png}[m
[32m+[m[32m    \includegraphics[width=1\textwidth]{sparse_rgg.png}[m
     \caption{Random Geometric Graph with $n=1000$ uniformly sampled points, $h=\sqrt{\frac{\log(n)}{n}}$}[m
     \label{Rgg_fig}[m
 \end{figure}[m
 [m
[31m-\paragraph{Latent Position Models} The Latent Position Model was introduced by (\cite{Hoff}).[m
[31m-For a positive integer $n$, a  kernel $k$ on $\mathbb{R}^d$ taking values between $0$ and $1$ and a density $p$ on $\mathbb{R}^d$ the Latent Position Model $\LPM(n,k,p)$ generates edges between vertices of $[n]$ in two stages; first a sample of size $n$ of i.i.d. variables $X_1,...,X_n$ with density $p$ is drawn. The variable $X_i$ can be thought of as the position of node $i$ in the latent space. Next, given the sample $(X_1,...,X_n)$ edges are drawn independently with $P(i\sim j|X_i,X_j)=k(X_i,X_j)$. Intuitively this means that we are more likely to observe an edge between two nodes which have positions that are similar with respect to $k$. Both the Stochastic Block Model and the Random Geometric Graph can be thought of as a Latent Position Models with a suitable kernel $k$. If the kernel $k$ is stationary then the graph structure of the $LPM$ is in many respects similar to the graph structure of the Random Geometric Graph. On the other hand, The Stochastic Block Model can be represented as a Latent Position Model with $d=1$, with a kernel $k$ which can classify points that are arbitrarily close as being dissimilar. For the purposes of regression, the case where the kernel $k$ is stationary is a much more natural setup.[m
[31m-[m
[32m+[m[32m\subsection{Latent Position Models}[m
[32m+[m
[32m+[m[32mThe Latent Position Model (\cite{Hoff}) consists of three parameters:  the[m
[32m+[m[32mnumber of nodes $n$, a similarity kernel on[m
[32m+[m[32m$k\colon\mathbb{R}^d\times\mathbb{R}^d\to [0,1]$ and a probability density[m
[32m+[m[32mfunction $p$ on $\mathbb{R}^d$. It generates a random graph on $n$ nodes in two[m
[32m+[m[32mstages. First, a sample of i.i.d. variables $(X_1,...,X_n)$ with density $p$ is[m
[32m+[m[32mdrawn. The variable $X_i$ is called the latent position of node $i$. Second, a[m
[32m+[m[32mBernoulli variable with parameter $k(X_i,X_j)$ determines if there is an edge[m
[32m+[m[32mbetween nodes $i$ and $j$. Intuitively this means that we are more likely to[m
[32m+[m[32mobserve an edge between two nodes which have positions that are similar with[m
[32m+[m[32mrespect to $k$. Clearly the Random Geometric Graph is a special case of a Latent[m
[32m+[m[32mPosition Model \SB{explain}. A slightly less obvious fact is that the Stochastic Block Model is a special case of a Latent Position model too, with latent dimension $d=1$. We choose to work with the Latent Position Model of a random graph as a data generating process because it is sufficiently general to cover a wide range of random graphs models on one hand and it is simple enough for theoretical considerations on the other.[m[41m  [m
 \begin{figure}[h!][m
     \centering[m
[31m-    \includegraphics[width=0.75\textwidth]{lpm_image_correct.png}[m
[32m+[m[32m    \includegraphics[width=1\textwidth]{lpm_image_correct.png}[m
     \caption{Latent position model with $n=300$ nodes, Gaussian kernel, and bandwith $h=0.75(\frac{\log(n)}{n})^{1/2}$}[m
     \label{fig:LPM_plot}[m
 \end{figure}[m
 [m
 [m
[32m+[m[32m\subsection{Nonparametric regression and the Nadaraya-Watson estimator}[m
[32m+[m[32mBecause of the kernel-based data-generating process in the Latent Position[m
[32m+[m[32mModel, simple graph averaging is closely related to a popular weighted average[m
[32m+[m[32mestimator known as the Nadaraya Watson estimator \SB{too abstract, expand and explain}. This observation motivates the name \textit{Graphical Nadaraya Watson} estimator. In the classical nonparametric regression problem  we are given data points $X_1,...,X_n\in[0,1]^d$ which are either fixed\footnote{For example equally spaced points on the unit cube $[0,1]^d$} or independent samples with common density $p$ and[m[41m  [m
[32m+[m[32mnoisy observations $Y_i=f(X_i)+\epsilon_i$. Here, $f\colon [0,1]^d\to\mathbb{R}$ is an unknown function and in some suitable function class $\mathcal{F}$ and[m
[32m+[m[32m$\epsilon_1,...,\epsilon_n$ are assumed to be i.i.d. centered variables with finite variance $\sigma^2$. The goal is to estimate $f$. The term \textit{nonparametric} stems from the fact that the function class $\mathcal{F}$ can not be parametrized by a subset of $\mathbb{R}^m$ for any $m\in \mathbb{N}$.[m
[32m+[m[32mA basic idea for estimating $f$ at a point $x\in[0,1]^d$ is to average the observations $Y_i$ for which $|X_i-x|$ is smaller than a certain threshold. More precisely, given $h>0$ an intuitive estimator for $f(x)$ is[m
[32m+[m
[32m+[m[32m\begin{equation*}[m
[32m+[m[32m    \hat{f}_{h}(x)=\begin{cases}[m
[32m+[m[32m    \frac{\sum_{i=1}^nY_iI(|x-X_i|\leq h)}{\sum_{i=1}^nI(|x-X_i|\leq h)} \quad &\text{if}\, \sum_{i=1}^n[m[41m [m
[32m+[m[32m    I(|x-X_i|\leq h)>0\\[m
[32m+[m[32m    0 \quad &\text{otherwise}\\[m
[32m+[m[32m    \end{cases}[m
[32m+[m[32m\end{equation*}[m
[32m+[m[32mMore generally one can consider weighted averages which depend on the distance in a more sensitive manner so that the weights[m[41m  [m
[32m+[m
[32m+[m[32mThis is a special case of the \textit{Nadaraya Watson} estimator:[m
[32m+[m
[32m+[m[32m\begin{equation}[m
[32m+[m[32m\label{NW_def}[m
[32m+[m[32m    \hat{f}_{NW,h}(x)=\begin{cases}[m
[32m+[m[32m    \frac{\sum_{i=1}^nY_iK(\frac{x-X_i}{h})}{\sum_{i=1}^n{K(\frac{x-X_i}{h}})} \quad &\text{if}\, \sum_{i=1}^n[m[41m [m
[32m+[m[32m    K(\frac{x-X_i}{h})\neq 0\\[m
[32m+[m[32m    0 \quad &\text{otherwise}\\[m
[32m+[m[32m    \end{cases}[m
[32m+[m[32m\end{equation}[m
[32m+[m[32mThe parameter $h>0$ is called the bandwith.[m[41m [m
[32m+[m
[32m+[m[32m\begin{figure}[m
[32m+[m[32m    \centering[m
[32m+[m[32m    \includegraphics[width=0.45\textwidth]{low_bias_001.png}[m
[32m+[m[32m    \includegraphics[width=0.45\textwidth]{low_variance_06.png}[m
[32m+[m[32m    \caption{Bias-Variance Tradeoff: n=1000 points are sampled uniformly and[m
[32m+[m[32m      independently on $[0,1]$ and gaussian noise with variance $\sigma^2=1$ is[m
[32m+[m[32m      added. Left: $\hat{f}_{NW}$ estimator with $h=0.01$. Right: $\hat{f}_{NW}$[m
[32m+[m[32m      estimator with $h=0.6$ $\hat{f}_{NW}$. \SB{Good illustration, but fonts[m
[32m+[m[32m        are too small, axes are unlabeled, yellow curve is barely visible. Add a[m
[32m+[m[32m      comment saying left panel is high-variance, low-bias and right panel is[m
[32m+[m[32m      the opposite.}[m
[32m+[m[32m }[m
[32m+[m[32m    \label{fig_nw_sensitivity}[m
[32m+[m[32m\end{figure}[m
[32m+[m
[32m+[m[32m\begin{figure}[m
[32m+[m[32m    \centering[m
[32m+[m[32m    \includegraphics[width=0.45\textwidth]{bias_variance_tradeof_01.png}[m
[32m+[m[32m    \caption{Same setting as figure \ref{fig_nw_sensitivity}, $\hat{f}_NW$ estimator with $h=0.1$}[m
[32m+[m[32m    \label{fig_nw_bias_var_tradeoff}[m
[32m+[m[32m\end{figure}[m
[32m+[m
[32m+[m
[32m+[m[32mThe quality of the estimator in terms of the $L^2$-risk[m
[32m+[m[32m$E((\hat{f}_{NW}(x)-f(x))^2$ at a fixed point $x\in\mathbb{R}^d$ will depend on[m
[32m+[m[32mthe many factors, such as the regularity of $f$, the choice of the kernel $k$[m
[32m+[m[32mbut most importantly on the choice of bandwith $h>0$. The $L^2$-risk admits the so-called bias-variance decomposition[m
[32m+[m
[32m+[m[32m\begin{equation}[m
[32m+[m[32m\label{eqn:bias-variance-decomp}[m
[32m+[m[32mE(\hat{f}_{NW}(x)-f(x))^2=Var(\hat{f}_{NW}(x))+(E\hat{f}_{NW(x)}-f(x))^2[m
[32m+[m[32m\end{equation}[m
[32m+[m[32mThe quantity $E(\hat{f}_{NW}(x)-f(x))$ is known as ``bias'' and under regularity assumptions on $f$ such as $\alpha$-H√∂lder continuity, this term is upper bounded by $Ch^{\alpha}$ where $C$ depends on $k$ and $\alpha$ but not on the sample size $n$. We will give a more thorough discussion of the bias in Chapter 4. On the other hand, the variance $Var(\hat{f}_{NW}(x)$ tends to increase as $h$ decreases, this is the famous bias-variance tradeoff phenomenon. In the fixed design setting optimal rates for $h$ are available, and they are dependent on the sample size $n$ (see \cite{Tsybakov} section 1.5 and section 1.6, Proposition 1.13). The main takeaway is that the statistician should adapt $h$ with respect to the sample size $n$.[m[41m [m
 [m
[31m-\subsection{Framework}[m
 [m
[31m-\paragraph{Framework} We observe a random graph with vertex set $[n+1]$  sampled according to an $\LPM(n+1,k_n,p)$ and assume that for nodes $i=1,...n$ (all but the last node) there is a label of the form $Y_i=f(X_i)+\epsilon_i$ with $f\colon\mathbb{R}^d\to\mathbb{R}$. Besides the graph itself and the explanatory variables $Y_1,...,Y_n$ no other quantities are assumed to be known. We write $X$ in place of $X_{n+1}$ and for $i=1,...,n$ we write $a(X,X_i)$ for the indicator that there is an edge between the node $n+1$ and node $i$. To describe edge generation more precisely, we assume that for $i=1,...,n$ the indicator of an edge between $X$ and $X_{i}$ is given by [m
[32m+[m[32m \section{Framework}[m
[32m+[m[32m\paragraph{Framework} We observe a random graph with vertex set $[n+1]$  sampled[m
[32m+[m[32maccording to an $\LPM(n+1,k_n,p)$ and assume that for nodes $i=1,...n$ (all but[m
[32m+[m[32mthe last node) there is a label \SB{observation, label is for classification[m
[32m+[m[32m  problems exclusively} of the form $Y_i=f(X_i)+\epsilon_i$ with $f\colon\mathbb{R}^d\to\mathbb{R}$. Besides the graph itself and the explanatory variables $Y_1,...,Y_n$ no other quantities are assumed to be known. We write $X$ in place of $X_{n+1}$ and for $i=1,...,n$ we write $a(X,X_i)$ for the indicator that there is an edge between the node $n+1$ and node $i$. To describe edge generation more precisely, we assume that for $i=1,...,n$ the indicator of an edge between $X$ and $X_{i}$ is given by[m[41m [m
 [m
 \begin{equation*}[m
     a(X,X_{i})=I(U_i\leq k(X,X_i))[m
 \end{equation*}[m
[31m-where $U_1,...,U_n$ are uniform variables on $[0,1]$, such that $(U_1,...,U_n,X_1,...,X_n,X,\epsilon_1,....,\epsilon_n)$ are jointly independent variables. [m
[31m-[m
[31m-For $x\in\mathbb{R}^d$, we define[m
[32m+[m[32mwhere $U_1,...,U_n$ are uniform variables on $[0,1]$, such that $(U_1,...,U_n,X_1,...,X_n,X,\epsilon_1,....,\epsilon_n)$ are jointly independent variables. For $x\in\mathbb{R}^d$, we define[m
 [m
 \begin{equation*}[m
     a(x,X_i)=I(U_i\leq k(x,X_i))[m
 \end{equation*}[m
[31m-[m
 We introduce the \textbf{Graphical Nadaraya Watson} estimator given by [m
 \begin{equation}[m
 \label{gnw_def}[m
[36m@@ -147,95 +250,266 @@[m [mWe introduce the \textbf{Graphical Nadaraya Watson} estimator given by[m
     0 \quad &\text{otherwise}\\[m
 \end{cases}[m
 \end{equation}[m
[31m-We are interested in two types of risk which we consider as the main measure of performance of $\hat{f}_{GNW}(x)$.[m
[31m-[m
[31m-[m
[31m-\begin{itemize}[m
[31m-    \item For a fixed point $x\in\supp{p}$, we are interested in a bound on the risk at the point $x$ given by[m
[31m-\begin{equation}[m
[31m-\label{fixed_point_risk}[m
[31m-    E(\hat{f}_{GNW}(x)-f(x))^2[m
[31m-\end{equation}[m
[31m-    \item For the random variable $X$ which represents node $n+1$ we define the risk at the random point $X$[m
 [m
[31m-\begin{equation}[m
[31m-\label{random_point_risk}[m
[31m-    E(\hat{f}_{GNW}(X)-f(X))^2[m
[31m-\end{equation}[m
[31m-\end{itemize}[m
 [m
[31m-To our knowledge there are no results in on graph regression in this context.[m
[32m+[m[32mTo our knowledge there are no results on graph regression \SB{you haven't[m
[32m+[m[32m  defined graph regression} in this context.[m
 [m
 [m
[31m-\subsection{Expected local degrees}[m
[31m-We introduce the quantity [m
[32m+[m[32m\subsection{Definitions}[m
[32m+[m[32mA central quantity of interest for our considerations is the local connection parameter defined by[m
 \begin{equation}[m
 \label{c_n_eqn}[m
     c_n(x)=\int_{\mathbb{R}^d} k_n(x,z)p(z)dz[m
 \end{equation}[m
[31m-Heuristically speaking, when conditioned on $X=x$ the degree of $X$ is[m
[31m-$\sum_{i=1}^n a(x,X_i)$. Hence we define the expected local degree at $x$ with[m
[31m-[m
[32m+[m[32mNote that $c_n(x)=E(a(x,X_i))$ and $E(a(X,X_i)|X=x)=a(x,X_i)$. Moreover,[m
[32m+[m[32m\begin{equation*}[m
[32m+[m[32m    E(\sum_{i=1}^na(X,X_i)|X=x)=\sum_{i=1}^n a(x,X_i)[m
[32m+[m[32m\end{equation*}[m
[32m+[m[32mHence we define the expected local degree at $x$ with[m
 \begin{equation}[m
 \label{local_degree}[m
     d_n(x)=nc_n(x)[m
 \end{equation}[m
[32m+[m[32mWe define the integral operator $T_{k_n}(\cdot,x)$ on the set of bounded functions  $f\colon\mathbb{R}^d\to\mathbb{R}$ by[m
[32m+[m[32m\begin{equation*}[m
[32m+[m[32m    T_{k_n}(f,x)=\int f(z)k_n(x,z)p(z)dz[m
[32m+[m[32m\end{equation*}[m
[32m+[m[32mNote that $c_n(x)=T_{k_n}(1,x)$. Lastly, we define \SB{explain how the quantity[m
[32m+[m[32m  below relates to NW estimator and why it's called $b_n$}[m
[32m+[m
[32m+[m[32m\begin{equation*}[m
[32m+[m[32mb_n(f,x)=\begin{cases}[m
[32m+[m[32m    \frac{T_{k_n}(f,x)}{c_n(x)} \quad &\text{if}\, c_n(x)>0\\[m
[32m+[m[32m    0 \quad &\text{otherwise}\\[m
[32m+[m[32m\end{cases}[m
[32m+[m[32m\end{equation*}[m
[32m+[m[32mNote that if $c_n(x)=0$, then $b_n(f,x)=0$ by definition, while on the other hand for all $i=1,2,...,n$, $a(x,X_i)=0$, and hence $\hat{f}_{GNW}(x)=0$. Therefore the variance term at $x$ is nontrivial only at points $x\in\mathbb{R}^d$ with $c_n(x)>0$.[m
[32m+[m
[32m+[m[32m\SB{This bit looks unfinished}[m
[32m+[m
[32m+[m[32m\subsection{Outline}[m
[32m+[m
[32m+[m[32m\subsection{Concentration properties and variance term}[m
[32m+[m
[32m+[m[32m\SB{What's a concentration property? What problem are we solving here? }[m
[32m+[m
[32m+[m[32mIn section \ref{concentration_properties} we show that the if the noise variables $\epsilon_i$, $i=1,2,3,....,n$ are bounded then graph averaging estimator at $x$ towards $b_n(f,x)$ with rate $O(\exp(-\delta^2d_n(x))$. We also discuss the case for gaussian noise.[m
 [m
[31m-\subsection{Classical and Graphical Nadaraya Watson estimators: similarities and differences}[m
[32m+[m[32m\subsection{Variance term}[m
[32m+[m
[32m+[m[32m\SB{Instead of saying ``we introduce'', explain why the variance term is[m
[32m+[m[32m  important and fits into the overall scheme of things}[m
[32m+[m[32mWe introduce the variance term at $x\in\mathbb{R}^d$ by[m[41m [m
 [m
[31m-Because of the kernel based data generating process in the Latent position model, the Graphical Nadaraya Watson Estimator has strong connections with the classical Nadaraya Watson estimator. We emphasize the fact that while in classical nonparametric estimation the choice of the kernel is up to the statistician, in our setup the kernel $k_n$ is given with the Latent Position Model, and hence it represents an oracle quantity. We define the integral operator $T_{k_n}(\cdot,x)$ on the set of bounded functions  $f\colon\mathbb{R}^d\to\mathbb{R}$ by[m
 \begin{equation}[m
[31m-\label{T_n_eqn}[m
[31m-    T_k(f,x)==\int_{\mathbb{R}^d}f(z)k_n(x,z)p(z)dz[m
[32m+[m[32m\label{variance_term}[m
[32m+[m[32m    E((\hat{f}_{GNW}(x)-b_n(f,x))^2[m
 \end{equation}[m
[31m-It is easy to see that [m
[32m+[m
[32m+[m[32m\begin{equation*}[m
[32m+[m[32m    P(|\hat{f}_{GNW}(x)-b_n(f,x)|\geq \delta)\leq c_1e^{-\delta^2d_n(x)}[m
[32m+[m[32m\end{equation*}[m
[32m+[m[32mIn section \ref{bounding_the_variance} we show that there are constants $c_1,c_2>0$ such that[m[41m [m
[32m+[m
[32m+[m[32m\begin{equation*}[m
[32m+[m[32m    \frac{c_1}{d_n(x)}\leq E(\hat{f}-b_n(f,x))^2\leq \frac{c_2}{d_n(x)}[m
[32m+[m[32m\end{equation*}[m
[32m+[m
[32m+[m
[32m+[m[32m\subsection{Bias}[m
[32m+[m
[32m+[m[32m\SB{Explain that what's being studied here is independent of the graph, and more[m
[32m+[m[32m  related to the regularity of the signal $f$ - how far it is from its own local[m
[32m+[m[32m  average}[m
[32m+[m
[32m+[m[41m  [m
[32m+[m[32mWe introduce the bias term at $x\in\supp{p}$ by[m
[32m+[m
 \begin{equation}[m
[31m-    T_{k_n}(f,x)=Ef(X_i)a(x,X_i)[m
[31m-\end{equation}[m
[31m-Note also that $c_n(x)=T_{k_n}(1,x)$.[m
[31m-As a rough heuristic\footnote{If $k_n$ did not depend on $n$ this heuristic would be a fact, due to the law of large numbers} to motivate further discussion, we expect that $\frac{1}{n}\sum_{i=1}f(X_i)a(x,X_i)-T_{k_n}(f,x)\to 0$. As $\hat{f}_{GNW}(x)$ is a quotient of $\frac{1}{n}\sum_{i=1}^n Y_ia(x,X_i)$ and $\frac{1}{n}\sum_{i=1}^na(x,X_i)$[m
[31m-which are close to $T_{k_n}(f,x)$ and $c_n(x)$ respectively, it is reasonable to expect that $\hat{f}_{GNW}(x)$ will be close to[m
[32m+[m[32m\label{bias_term}[m
[32m+[m[32m    |b_n(f,x)-f(x)|[m
[32m+[m[32m  \end{equation}[m
[32m+[m[32m\SB{Usually (and by your own definition above) a bias is a signed quantity, not an absolute value}[m[41m  [m
[32m+[m[32mThe study of the bias term reduces to a study of convergence rate of a certain integral operator.[m
[32m+[m[32mIt is easy to control the variance term and the bias term separately, but requiring control over both of them at the same time significantly restricts the function class, the kernel assumptions and the density assumptions. We will restrict our attention to the geometric setting where the kernel $k_n$ is of the form $k_n(x,z)=K(\frac{x-z}{h_n})$ which will mimic the properties of the Random Geometric Graph. Pointwise results in this setting are still easy to obtain.[m
[32m+[m[32m\ref{uniform_bias_control} we show that[m
 [m
[32m+[m[32m\begin{equation*}[m
[32m+[m[32m    sup_{x\in\supp{p}}|f(x)-b_n(f,x)|\leq Ch^{\alpha}_n[m
[32m+[m[32m\end{equation*}[m
[32m+[m[32m\SB{Use the DeclareMathOperator command for sup, argmax, etc. Shouldn't be in[m
[32m+[m[32m  italic. }[m
[32m+[m[32m\subsection{Risk}[m
[32m+[m[32mWe study two types of risk:[m
[32m+[m[32m\begin{itemize}[m
[32m+[m[32m    \item For a fixed point $x\in\supp{p}$, we consider the risk at the point $x$ given by[m
[32m+[m[32m\begin{equation}[m
[32m+[m[32m\label{fixed_point_risk}[m
[32m+[m[32m    E(\hat{f}_{GNW}(x)-f(x))^2[m
[32m+[m[32m  \end{equation}[m
[32m+[m[32m  \SB{Be careful, you have unbalanced brackets all over the place, I'm not going[m
[32m+[m[32m  to fix them all}[m
[32m+[m[32m    \item For the random variable $X$ which represents node $n+1$ we define the risk at the random point $X$[m
 \begin{equation}[m
[31m-    b_n(f,x)=\frac{T_{k_n}(f,x)}{c_n(x)}[m
[32m+[m[32m\label{random_point_risk}[m
[32m+[m[32m    E(\hat{f}_{GNW}(X)-f(X))^2[m
 \end{equation}[m
[31m-Note that the same rough heuristic applies to the classical Nadaraya Watson estimator by replacing $a(x,X_i)$ by $k_n(x,X_i)$.[m
[31m-[m
[31m-\subsection{Strategy}[m
[32m+[m[32m\end{itemize}[m
 [m
 The risk at the point $x$ (\ref{fixed_point_risk}) is much easier to control compared to the risk at the random point $X$ (\ref{random_point_risk}), so first we give the main ideas on how to bound (\ref{fixed_point_risk}) from above. The strategy is to decompose Equation (\ref{fixed_point_risk}) into a variance term[m
 [m
[32m+[m[32m\section{Concentration properties}[m
[32m+[m[32m\label{concentration_properties}[m
[32m+[m[32mAs a rough heuristic to motivate further discussion, we could attempt to use concentration inequalities for the numerator and the denominator separately, i.e. it is reasonable to expect that with high probability, $\frac{1}{n}\sum_{i=1}^n Y_ia(x,X_i)$ will be close to $T_{k_n}(f,x)$ and with high probability $\frac{1}{n}\sum_{i=1}^na(x,X_i)$ will be close to $c_n(x)$. Once these heuristics are formalized as concentration inequalities, the results follow from a union bound. We do not choose to pursue this approach because it is a bit tedious, mainly due to the fact that $d_n(x)$ is the correct scaling instead of $n$. This is easy to justify heuristically, as we see roughly $d_n(x)$ terms in the sums $\sum_{i=1}^n Y_ia(x,X_i)$. In Lemma \ref{bernstein_corollary} we prove a deviation bound on $\sum_{i=1}^n a(x,X_i)-d_n(x)$. In Lemma \ref{basic_lemma_1} we show that with high probability  $\frac{1}{d_n(x)}\sum_{i=1}f(X_i)a(x,X_i)$ is close to $\frac{\sum_{i=1}^na(x,X_i)}{d_n(x)}b_n(f,x)$. It is a bit harder to see why this is true at the first glance. Finally, we need to show that  $\frac{\sum_{i=1}^n\epsilon_ia(x,X_i)}{\sum_{i=1}a(x,X_i)}$ concentrates towards zero at a sufficiently fast rate. This is where the assumptions on the noise come in to play, depending on the distribution on the noise different regimes apply.[m[41m [m
[32m+[m
[32m+[m[32m\begin{lemma}[m
[32m+[m[32m\label{bernstein_corollary}[m
[32m+[m[32m\begin{equation*}[m
[32m+[m[32m    P(|\sum_{i=1}^na(x,X_i)-d_n(x)|\geq \frac{d_n(x)}{2})\leq e^{-\frac{3d_n(x)}{14}}[m[41m [m
[32m+[m[32m\end{equation*}[m
[32m+[m[32m\end{lemma}[m
[32m+[m
[32m+[m[32m\begin{proof}[m
[32m+[m[32mWe apply Bernstein's inequality for bounded distributions (\cite{vershynin} Theorem 2.8.4, page 39) on the variables $a(x,X_i)-c_n(x)$. For all $i=1,2,...,n$ we have[m
[32m+[m
[32m+[m[32m\begin{equation*}[m
[32m+[m[32m    E(a(x,X_i)-c_n(x))^2=c_n(x)(1-c_n(x)\leq c_n(x)[m
[32m+[m[32m\end{equation*}[m
[32m+[m[32mHence[m
[32m+[m[32m\begin{equation*}[m
[32m+[m[32m    \sum_{i=1}^n E(a(x,X_i)-c_n(x))^2\leq nc_n(x)=d_n(x)[m
[32m+[m[32m\end{equation*}[m
[32m+[m[32mSetting $t=\frac{d_n(x)}{2}$ we get[m
[32m+[m
[32m+[m[32m\begin{equation*}[m
[32m+[m[32m\begin{split}[m
[32m+[m[32m    P(|\sum_{i=1}^n a(x,X_i)-d_n(x)|\geq \frac{d_n(x)}{2})&\leq 2\exp(-\frac{d^2_n(x)/4}{d_n(x)+d_n(x)/6}\\[m
[32m+[m[32m    &=2\exp(-3d_n(x)/14)[m
[32m+[m[32m\end{split}[m
[32m+[m[32m\end{equation*}[m
[32m+[m[32m\end{proof}[m
[32m+[m
[32m+[m
[32m+[m
[32m+[m[32m\begin{lemma}[m
[32m+[m[32m\label{basic_lemma_1}[m
[32m+[m[32mSuppose that $f$ is bounded, measurable function with  $||f||_{\infty}\leq B$. Then[m[41m [m
[32m+[m[32m\begin{equation*}[m
[32m+[m[32mP(|\frac{\sum_{i=1}^nf(X_i)a(x,X_i)}{d_n(x)}-\frac{\sum_{i=1}^na(x,X_i)}{d_n(x)}b_n(f,x)|\geq \delta)\leq 2\exp(-\frac{2\delta^2d_n(x)}{4B^2+B\delta/3})[m
[32m+[m[32m\end{equation*}[m
[32m+[m[32m\end{lemma}[m
[32m+[m[32m\begin{proof}[m
[32m+[m[32mIt is easy to see that the variables  $[f(X_i)-b_n(f,x)]a(x,X_i)$, $i=1,2,3,...,n$ are i.i.d., centered, bounded in absolute value by $2B$ and the sum of their variances satisfies[m[41m [m
[32m+[m[32m\begin{equation*}[m
[32m+[m[32m\begin{split}[m
[32m+[m[32m    \sum_{i=1}^nE([f(X_i)-b_n(f,x)]a(x,X_i))^2&=nE([f(X_1)-b_n(f,x)]^2a(x,X_i))\\[m
[32m+[m[32m    &\leq 4nB^2c_n(x)=4B^2d_n(x)[m
[32m+[m[32m\end{split}[m
[32m+[m[32m\end{equation*}[m
[32m+[m[32mBy Bernstein's inequality for bounded distributions applied to the centered variables $[f(X_i)-b_n(f,x)]a(x,X_i)$ we get[m
[32m+[m[32m\begin{equation*}[m
[32m+[m[32m    P(|\sum_{i=1}^n[f(X_i)-b_n(f,x)]a(x,X_i)|\geq t)\leq 2\exp(\frac{-t^2/2}{4B^2d_n(x)+Bt/3})[m
[32m+[m[32m\end{equation*}[m
[32m+[m[32mSubstituting $t=\delta d_n(x)$ gives the desired result.[m
[32m+[m[32m\end{proof}[m
[32m+[m[32mThe following result is yet another \SB{similar? explains why you don't give a[m
[32m+[m[32m  proof} application of Bernstein's inequality.[m
[32m+[m[32m\begin{lemma}[m
[32m+[m[32m\label{basic_lemma_2}[m
[32m+[m[32mSuppose that the noise variables are bounded and centered, i.e. $|\epsilon_i|\leq \sigma<\infty$. Then[m
[32m+[m
[32m+[m[32m\begin{equation*}[m
[32m+[m[32m    P(|\frac{\sum_{i=1}^n \epsilon_{i}a(x,X_i)}{d_n(x)}|\geq \delta)\leq \exp(-3\delta^2d_n(x)/(2\sigma+6\sigma^2))[m
[32m+[m[32m\end{equation*}[m
[32m+[m[32m\end{lemma}[m
[32m+[m[32mCombining the basic lemmas from this section, we get the following result.[m
[32m+[m[32m\begin{theorem}[m
[32m+[m[32m\label{basic_cor_2}[m
[32m+[m[32mSuppose that $f$ is a bounded and measurable function with $||f||\leq B$, the noise variables are bounded i.e. $|\epsilon_i|\leq \sigma^2$ and $d_n(x)>0$. Then for all $\delta>0$[m
[32m+[m[32m\begin{equation*}[m
[32m+[m[32m    P(|\hat{f}_{GNW}(x)-b_n(f,x)|\geq \delta)\leq 6\exp(-C(\delta,B,\sigma)d_n(x))[m
[32m+[m[32m\end{equation*}[m
[32m+[m[32mwhere $C(\delta,B,\sigma)=\min\{3/14,3\delta^2/(32\sigma+96\sigma^2),6\delta^2/(192B^2+\delta B))$[m
[32m+[m[32m\end{theorem}[m
[32m+[m[41m [m
[32m+[m[32m\begin{proof}[m
[32m+[m[32mSuppose that[m
 \begin{equation}[m
[31m-\label{variance_term}[m
[31m-E(\hat{f}_{GNW}(x)-b_n(f,x))^2[m
[32m+[m[32m\label{deg_cond}[m
[32m+[m[32m    \sum_{i=1}^n a(x,X_i)\geq d_n(x)/2[m
 \end{equation}[m
[31m-and a bias term[m
[32m+[m[32mNote that in particular this implies $\sum_{i=1}^n a(x,X_i)>0$.[m
[32m+[m[32mThen[m
 \begin{equation}[m
[31m-\label{bias_term}[m
[31m-b_n(f,x)-f(x)[m
[32m+[m[32m\begin{split}[m
[32m+[m[32m    |\hat{f}_{GNW}(x)-b_n(f,x)|&=\frac{|\sum_{i=1}^nf(X_i)a(x,X_i)-b_n(f,x)\sum_{i=1}^n a(x,X_i)|}{\sum_{i=1}^n a(x,X_i)}|+\frac{|\sum_{i=1}^n \epsilon_ia(x,X_i)|}{\sum_{i=1}^da(x,X_i)}\\[m
[32m+[m[32m    &\leq \frac{2|\sum_{i=1}^nf(X_i)a(x,X_i)-b_n(f,x)\sum_{i=1}^n a(x,X_i)|}{d_n(x)}+\frac{2|\sum_{i=1}^n \epsilon_ia(x,X_i)|}{d_n(x)}[m
[32m+[m[32m\end{split}[m
[32m+[m[32m\end{equation}[m
[32m+[m[32mIf in addition[m
[32m+[m[32m\begin{equation}[m
[32m+[m[32m\label{weird_cond}[m
[32m+[m[32m|\frac{\sum_{i=1}^nf(X_i)a(x,X_i)}{d_n(x)}-\frac{\sum_{i=1}^na(x.X_i)}{d_n(x)}b_n(f,x)|<\delta/4[m
[32m+[m[32m\end{equation}[m
[32m+[m[32mand[m
[32m+[m[32m\begin{equation}[m
[32m+[m[32m\label{noise_cond}[m
[32m+[m[32m|\sum_{i=1}^n \epsilon_ia(x,X_i)|<\delta/4[m
 \end{equation}[m
[31m-and bound those terms separately. The tools used to the variance term (\ref{variance_term}) are probabilistic in nature such as decoupling arguments and concentration inequalities. On the other hand the tools needed  to bound the bias term (\ref{bias_term}) are of geometric nature, as they basically ammount to studying convergence of certain integral operators towards the identity. We will show that in a general Latent position model the variance term (\ref{variance_term}) satisfies[m
[32m+[m[32mThen we get[m[41m [m
[32m+[m[32m\begin{equation*}[m
[32m+[m[32m|\hat{f}_{GNW}(x)-b_n(f,x)|<\delta[m
[32m+[m[32m\end{equation*}[m
[32m+[m[32mHence if $|\hat{f}_{GNW}(x)-b_n(f,x)|\geq \delta$ then at least one of the inequalities \ref{deg_cond}, \ref{weird_cond} or \ref{noise_cond} must be violated. We conclude by using Lemmas \ref{bernstein_corollary}, \ref{basic_lemma_1} and \ref{basic_lemma_2} together with a union bound[m[41m [m
 [m
 \begin{equation*}[m
[31m-    \frac{C_1(\sigma^2)(1-e^{-d_n(x)})}{d_n(x))}\leq E(\hat{f}_{GNW}(x)-b_n(f,x))^2\leq \frac{C_2(B,\sigma^2)}{d_n(x})[m
[32m+[m[32m\begin{split}[m
[32m+[m[32m    P(|\hat{f}_{GNW}(x)-b_n(f,x)|\geq\delta)&\leq P(|\sum_{i=1}^n a(x,X_i)-d_n(x)|\geq d_n(x)/2)\\[m
[32m+[m[32m    &+P(|\frac{\sum_{i=1}^nf(X_i)a(x,X_i)}{d_n(x)}-\frac{b_n(f,x)\sum_{i=1}^na(x,X_i)}{d_n(x)}|\geq \delta/4)\\[m
[32m+[m[32m    &+P(|\frac{\sum_{i=1}^n \epsilon_i a(x,X_i)}{d_n(x)}|\geq \delta/4)\\[m
[32m+[m[32m    &\leq 6\exp(-C(\delta,B,\sigma)d_n(x))[m
[32m+[m[32m\end{split}[m
 \end{equation*}[m
[31m-where $d_n(x)$ is the local average degree given by Equation (\ref{local_degree}). On the other hand the bias in a general Latent position model may behave poorly. We will restrict our attention to the geometric setting where the kernel $k_n$ is of the form $k_n(x,z)=\rho_n\frac{K(x-z}{h_n}$ which will mimic the properties of the Random Geometric Graph. Pointwise results in this setting are still easy to obtain. Finally, once we establish upper bounds on Equation (\ref{bias_term}) and (\ref{variance_term}) we may integrate them against the $p$ to obtain bounds for the risk at the random point $X$. This strategy requires strong assumption on the distribution in order to work.  [m
[32m+[m[32mwhere $C(\delta,B,\sigma)=\min\{3/14,3\delta^2/(32\sigma+96\sigma^2),6\delta^2/(192B^2+\delta B))$[m
[32m+[m
[32m+[m[32m\end{proof}[m
[32m+[m
[32m+[m
[32m+[m
[32m+[m[32m\subsection{Remarks}[m
[32m+[m
[32m+[m[32m\begin{remark}[m
[32m+[m[32mCorollary \SB{you've called it a theorem I think} \ref{basic_cor_2} states that as long as the local expected degree at $x$ \ref{local_degree} grows to infinity, $\hat{f}_{GNW}(x)-b_n(f,x)\to 0$ in probability. In particular there is no requirement on the speed of growth of $d_n(x)$ to ensure convergence in probability. On the other hand, if $d_n(x)\geq r\log(n)$ for some $r>1$, then the left hand side of the inequality in Corollary \ref{basic_cor_2} is summable, so the Borel Cantelli lemma implies that $\hat{f}_{GNW}(x)-b_n(f,x)\to 0$ almost surely.[m
[32m+[m[32m\end{remark}[m
 [m
[31m-\paragraph{Outline}[m
[32m+[m[32m\begin{remark}[m
[32m+[m[32mThe fact that the noise variables $\epsilon_1,...,\epsilon_n$ are bounded was[m
[32m+[m[32mcrucial in obtaining such a strong bound as in Corollary \ref{basic_cor_2}. If[m
[32m+[m[32mwe assume that the noise is Gaussian, which is standard in statistics, we get a[m
[32m+[m[32msignificantly worse bound on the probability appearing in \ref{basic_lemma_2},[m
[32m+[m[32mnamely one of order $O(\exp(-c(\delta,B,\sigma)d_n^2(x)/n))$. This limitation is[m
[32m+[m[32mdue to the concentration inequalities developed for gaussian variables. It[m
[32m+[m[32mstates that with gaussian noise, convergence in probability in ensured only in[m
[32m+[m[32mthe case when we have $d_n(x)/\sqrt{n}\to\infty$, i.e. where the expected local[m
[32m+[m[32mdegree at $x$ grows faster than $\sqrt{n}$. \SB{This feels strange to me, if you[m
[32m+[m[32mhave unit-variance Gaussian noise then with high prob. it's bounded by some[m
[32m+[m[32mquantity that depends only weakly on $n$. It doesn't seem like there should be[m
[32m+[m[32msuch a large difference between bounded noise and Gaussian noise. }[m
[32m+[m[32m\end{remark}[m
[32m+[m
[32m+[m[32m\SB{Give a little summary here. What have we learned, and what do we have left[m
[32m+[m[32m  to do?}[m
 [m
 \section{Bounding the variance at a point}[m
[31m-\subsection{Computing  $E\hat{f}_{GNW}(x)$}[m
[31m-The risk at the point $x$ (\ref{fixed_point_risk}) admits the decomposition[m
[31m-\begin{equation}[m
[31m-\label{eqn_bias_variance}[m
[31m-    E(\hat{f}_{GNW}(x)-f(x))^2=Var(\hat{f}_{GNW}(x))+(E\hat{f}_{GNW}(x)-f(x))^2[m
[31m-\end{equation}[m
[31m-Equation (\ref{eqn_bias_variance}) is known as the bias-variance decomposition of risk. If we want asymptotically vanishing risk at the point $x$, i.e. [m
[31m-$E(\hat{f}_{GNW}(x)-f(x))^2\rightarrow 0$ as $n\rightarrow\infty$, we need  $E(\hat{f}_{GNW}(x))\rightarrow f(x)$ as $n\rightarrow\infty$. Thus it is of basic interest to compute $E(\hat{f}_{GNW}(x))$, at least asymptotically. Being a quotient of two random variables, the exact value of [m
[31m-$E\hat{f}_{GNW}(x)$ may seem difficult to compute. [m
[31m-In this section we compute explicitly  $E\hat{f}_{GNW}(x)$ for all $x$ such that $c_n(x)>0$. This is done via a decoupling argument. The method used here will be used again to bound the risk at the point $x$.[m
 [m
[31m-\paragraph{Decoupling argument}[m
[32m+[m[32m\SB{Need intro paragraph. Why do we care about the variance and what's the[m
[32m+[m[32m  strategy for bounding it? }[m
[32m+[m[32m\label{bounding_the_variance}[m
[32m+[m[32m\subsection{Computing  the expectation }[m
[32m+[m[32mBeing a quotient of two random variables, the exact value of $E\hat{f}_{GNW}(x)$ may seem difficult to compute. In this section we compute explicitly  $E\hat{f}_{GNW}(x)$ for all $x$ such that $c_n(x)>0$. This is done via a decoupling trick, i.e. a method which represents $\hat{f}_{GNW}(x)$ as a linear combination of fractions where the numerator and denominator of each fraction are independent. The method used here will be used again to bound the variance term at the point $x$ \ref{variance_term}[m
[32m+[m
[32m+[m[32m\paragraph{Decoupling trick}[m
 [m
 Let $I\subseteq [n]$.[m
 For $I=\emptyset$ we define[m
[36m@@ -251,7 +525,15 @@[m [mR_{\emptyset}(x)=\begin{cases}[m
 \begin{equation*}[m
     R_I(x)=\frac{1}{|I|+\sum_{j\notin I}a(x,X_j)}[m
 \end{equation*}[m
[31m-For convenience of notation we write $R_i(x)=R_{\{i\}}(x)$.[m
[32m+[m[32mFor convenience of notation we write[m
[32m+[m[32m\begin{equation*}[m
[32m+[m[32m    R_i(x)=R_{\{i\}}(x)[m
[32m+[m[32m\end{equation*}[m
[32m+[m[32mand[m[41m [m
[32m+[m[32m\begin{equation}[m
[32m+[m[32m\label{shorthand}[m
[32m+[m[32m    Z=I(\sum_{i=1}^n a(x,X_i)>0)[m
[32m+[m[32m\end{equation}[m
 Note that for all pairs of disjoint subsets $I,J\subseteq [n]$ we have[m
 [m
 \begin{equation}[m
[36m@@ -277,7 +559,7 @@[m [mNote that $R_i(x)$, $i=1,2,...,n$ are identically distributed, hence  $ER_i(x)=E[m
 By summing Equations (\ref{eqn_R_single}) for $i=1,2,3,...,n$ we have [m
 \begin{equation}[m
 \label{eqn_for_Ri}[m
[31m-    \sum_{i=1}^n a(x,X_i)R_i(x)=R_{\emptyset}(x)\sum a(x,X_i)=I(\sum_{i=1}^n a(x,X_i)>0)[m
[32m+[m[32m    \sum_{i=1}^n a(x,X_i)R_i(x)=R_{\emptyset}(x)\sum_{i=1}^n a(x,X_i)=I(\sum_{i=1}^n a(x,X_i)>0)=Z[m
 \end{equation}[m
 Taking expectation and using the fact that $R_i(x)$ and $a(x,X_i)$ are independent, we get[m
 [m
[36m@@ -292,9 +574,8 @@[m [mTaking expectation and using the fact that $R_i(x)$ and $a(x,X_i)$ are independe[m
 On the other hand,[m
 \begin{equation}[m
 \label{eqn_for_Rempty}[m
[31m-    P(\sum_{i=1}^n a(x,X_i)>0)=1-P(\sum_{i=1}^n a(x,X_i)=0)=1-(1-c_n(x))^n[m
[32m+[m[32m    EZ=P(\sum_{i=1}^n a(x,X_i)>0)=1-P(\sum_{i=1}^n a(x,X_i)=0)=1-(1-c_n(x))^n[m
 \end{equation}[m
[31m-[m
 The result follows by combining Equations (\ref{eqn_for_Ri}), (\ref{eqn_for_ERi}) and (\ref{eqn_for_Rempty}).[m
 [m
 \end{proof}[m
[36m@@ -327,44 +608,84 @@[m [mHence, taking expectation and using Lemma  \ref{lemma_exp}, we get[m
 [m
 \end{proof}[m
 [m
[32m+[m[32m\SB{Here I think you need to expand your remarks a bit. Corollary 4.2 is[m
[32m+[m[32m  important and tells us that on average, what we estimate is like a local[m
[32m+[m[32m  average but with extra shrinkage towards 0. We therefore expect it to have[m
[32m+[m[32m  more bias than $b_n$ especially if $f$ has a high avg. value over the domain.[m
[32m+[m[32m  However, in high density regions, the extra bias disappears quite fast.[m[41m [m
[32m+[m[32m}[m
[32m+[m[41m  [m
[32m+[m[32m\begin{remark}[m
[32m+[m[32m  \SB{A better way of presenting such a result is to start with the conclusion[m
[32m+[m[32m    rather than the calculation (people will be tempted to skip the whole remark[m
[32m+[m[32m  otherwise)}[m
[32m+[m[32m  Consider $f$ to be the constant function $1$. Then $b_n(f,x)=1$ (recall the comments about the expected local degree (\ref{local_degree})). If $d_n(x)\leq d_0$ then $(1-\frac{d_n(x)}{n})^n\geq (1-\frac{d_0}{n})^n$ and consequently[m
[32m+[m[32m\begin{equation*}[m
[32m+[m[32m    |f(x)-E\hat{f}_{GNW}(x)|=|(1-\frac{d_n(x)}{n})^n|\geq (1-\frac{d}{n})^n[m
[32m+[m[32m\end{equation*}[m
[32m+[m[32mand in particular[m[41m [m
[32m+[m
[32m+[m[32m\begin{equation*}[m
[32m+[m[32m    \liminf_{n\to\infty}{|f(x)-E\hat{f}_{GNW}(x)|}\geq 1-e^{-d_0}[m
[32m+[m[32m\end{equation*}[m
[32m+[m[32mThus in the bounded degree case, even for the simplest functions $\hat{f}_{GNW}(x)$ is not asymptotically unbiased.[m
[32m+[m
[32m+[m[32m\end{remark}[m
 [m
 [m
 \subsection{The decoupling argument}[m
[31m-In the previous section we found an explicit expression for $E\hat{f}_{GNW}(x)$ (Corollary \ref{expectation_comp}). While this makes computation of $Var(\hat{f}_{GNW}(x))=E(\hat{f}_{GNW}(x)-E\hat{f}_{GNW}(x))^2$ possible, we find that it is much simpler to analyse $E(\hat{f}_{GNW}(x)-b_n(f,x))^2$ instead. Note that by definition[m
[32m+[m[32mIn the previous section we found an explicit expression for $E\hat{f}_{GNW}(x)$ (Corollary \ref{expectation_comp}). While this makes computation of $Var(\hat{f}_{GNW}(x))=E(\hat{f}_{GNW}(x)-E\hat{f}_{GNW}(x))^2$ possible, we find that it is much simpler to analyse $E(\hat{f}_{GNW}(x)-b_n(f,x))^2$ instead. Note that by definition (\ref{gnw_def}) and Equation (\ref{shorthand})[m
 [m
 \begin{equation}[m
 \label{gnw_no_edges}[m
[31m-    I(\sum_{i=1}^na(x,X_i)=0)\hat{f}_{GNW}(x)=0[m
[32m+[m[32m (1-Z)\hat{f}_{GNW}(x)=0[m
 \end{equation}[m
 or equivalently[m
 \begin{equation}[m
 \label{gnw_edges}[m
[31m-   I(\sum_{i=1}^na(x,X_i)>0)\hat{f}_{GNW}(x)=\hat{f}_{GNW}(x) [m
[32m+[m[32m   Z\hat{f}_{GNW}(x)=\hat{f}_{GNW}(x)[m[41m [m
 \end{equation}[m
[31m-Thus [m
[31m-[m
[32m+[m[32mKeeping in mind that $Z$ is an indicator of an event, we have $Z^2=Z$ and $(1-Z)^2=1-Z$, so using Equations (\ref{gnw_no_edges}, \ref{gnw_edges}), we get[m[41m [m
 \begin{equation}[m
 \label{variance_decomp}[m
 \begin{split}[m
[31m-    E(\hat{f}_{GNW}(x)-b_n(f,x))^2&=E[(\hat{f}_{GNW}(x)-b_n(f,x))^2I(\sum_{i=1}^na(x,X_i)>0))]\\[m
[31m-    &+E[(\hat{f}_{GNW}(x)-b_n(f,x))^2I(\sum_{i=1}^na(x,X_i)=0)]\\[m
[31m-    &=E(\hat{f}_{GNW}(x)-b_n(f,x)I(\sum_{i=1}^na(x,X_i)>0))^2+b_n^2(f,x)P(\sum_{i=1}^na(x,X_i)=0)[m
[32m+[m[32m    E(\hat{f}_{GNW}(x)-b_n(f,x))^2&=E[(\hat{f}_{GNW}(x)-b_n(f,x))^2Z))]+E[(\hat{f}_{GNW}(x)-b_n(f,x))^2(1-Z)]\\[m
[32m+[m[32m    &=E(\hat{f}_{GNW}(x)Z-b_n(f,x)Z)^2+E(\hat{f}_{GNW}(x)(1-Z)-b_n(f,x)(1-Z))^2\\[m
[32m+[m[32m    &=E(\hat{f}_{GNW}(x)-b_n(f,x)Z)^2+b_n^2(f,x)E(1-Z)\\[m
[32m+[m[32m    &=E(\hat{f}_{GNW}(x)-b_n(f,x)Z)^2+b_n^2(f,x)((1-c_n(x))^n)[m
 \end{split}[m
 \end{equation}[m
[32m+[m
[32m+[m[32m\SB{Please use backslash-left( or square/curly brackets, it's getting hard to read what's being averaged over}[m
[32m+[m
 Using Equation (\ref{eqn_R}) and Equation (\ref{eqn_for_Ri}) we have[m
 \begin{equation}[m
[31m-    \hat{f}_{GNW}(x)-b_n(f.x)I(\sum_{i=1}^n a(x,X_i)>0)=\sum_{i=1}^n(Y_i-b_n(f,x))a(x,X_i)R_i(x)[m
[32m+[m[32m\label{decomp}[m
[32m+[m[32m    \hat{f}_{GNW}(x)-b_n(f,x)Z=\sum_{i=1}^n(Y_i-b_n(f,x))a(x,X_i)R_i(x)[m
 \end{equation}[m
[31m-In Lemma \ref{trick_lemma_pt_1} we show that the summands in this representation are uncorrelated and consequently obtain tractable expression for $E(\hat{f}_{GNW}(x)-b_n(f.x)I(\sum_{i=1}^n a(x,X_i)>0))^2$. In contrast, computing the variance $Var(\hat{f}_{GNW}(x))$ directly  leaves a much more complicated expression.[m
[32m+[m[32mWe will show that the summands in the right hand side of Equation (\ref{decomp}) are uncorrelated and consequently we will obtain tractable expression for $E(\hat{f}_{GNW}(x)-b_n(f,x)Z)^2$. In contrast, computing the variance $Var(\hat{f}_{GNW}(x)$ leaves a tedious sum. We first state a preliminary lemma which follows easily from the decoupling trick (\ref{eqn_R}).[m
[32m+[m[32m\begin{lemma}[m
[32m+[m[32m\label{one_time_lemma}[m
[32m+[m[32mSuppose that $g\colon\mathbb{R}^{d+1}\to\mathbb{R}$ and is a  measurable function such that $g_1(X_1,\epsilon_1)\in\mathbb{L}^2$. For $1\leq i\leq n$ set $F_i=g(X_i,\epsilon_i)$. Then for all pairs of distinct indices $(i,j)$, $1\leq i,j \leq n$ we have[m
[32m+[m[32m\begin{equation*}[m
[32m+[m[32m    E(F_iF_ja(x,X_i)a(x,X_j)R_i(x)R_j(x))=E(F_ia(x,X_i))E(F_ja(x,X_j))E(R_{\{i,j\}}^2)[m
[32m+[m[32m\end{equation*}[m
[32m+[m[32m\end{lemma}[m
[32m+[m[32m\begin{proof}[m
[32m+[m
 [m
[32m+[m[32mUsing the decoupling trick (\ref{eqn_R}) we have[m
[32m+[m[32m\begin{equation*}[m
[32m+[m[32m    F_iF_ja(x,X_i)a(x,X_j)R_i(x)R_j(x)=F_iF_ja(x,X_i)a(x,X_j)R_{\{i,j\}}^2[m
[32m+[m[32m\end{equation*}[m
[32m+[m[32mand moreover $R_{\{i,j\}}$ is independent from $(X_i,\epsilon_i,a(x,X_i))$ and $(X_j,\epsilon_j,a(x,X_j))$. Next, $(X_i,\epsilon_i,a(x,X_i)$ and $(X_j,\epsilon_j,a(x,X_j))$ are also independent by modelling assumption. As independence implies uncorrelatedness, the conclusion follows.[m
[32m+[m[32m\end{proof}[m
 \begin{lemma}[m
 \label{trick_lemma_pt_1}[m
[31m-[m
 \begin{equation*}[m
[31m-    E(\hat{f}_{GNW}(x)-b_n(f,x)I(\sum_{i=1}^n a(x,X_i)>0))^2=[m
[31m-    n[E(f(X_1)-b_n(f,x))^2a(x,X_i)+\sigma^2c_n(x)]ER_1^2(x)[m
[32m+[m[32m    E(\hat{f}_{GNW}(x)-b_n(f,x)Z)^2=[m
[32m+[m[32m    n[E([f(X_1)-b_n(f,x)]^2a(x,X_i))+\sigma^2c_n(x)]ER_1^2(x)[m
 \end{equation*}[m
[31m-[m
 \end{lemma}[m
 \begin{proof}[m
 Using Equation (\ref{eqn_for_Ri}), we have[m
[36m@@ -372,22 +693,25 @@[m [mUsing Equation (\ref{eqn_for_Ri}), we have[m
 \begin{equation}[m
 \label{tricky_eqn}[m
 \begin{split}[m
[31m-    E(\hat{f}_{GNW}(x)-b_n(f,x)I(\sum_{i=1}^na(x,X_i)>0))^2&=E(\sum_{i=1}^n(Y_i-b_n(f,x))a(x,X_i)R_i(x))^2\\[m
[32m+[m[32m    E(\hat{f}_{GNW}(x)-b_n(f,x)Z)^2&=E(\sum_{i=1}^n(Y_i-b_n(f,x))a(x,X_i)R_i(x))^2\\[m
     &=\sum_{i=1}^n E(Y_i-b_n(f,x))a(x,X_i)R_i(x))^2\\[m
     &+\sum_{i\neq j}E((Y_i-b_n(f,x))(Y_j-b_n(f,x))a(x,X_i)a(x,X_j)R_i(x)R_j(x))[m
 \end{split}[m
 \end{equation}[m
[31m-For $i\neq j$, using Equation (\ref{eqn_R}), together with the fact that $R_{i,j}(x)$ is independent from  $Y_i,Y_j,a(x,X_i)$ and $a(x,X_j)$, as well as the fact that the pairs $(Y_i,a(x,X_i)$ and $(Y_j,a(x,X_j)$ are independent, we have [m
[32m+[m[32mFor $i\neq j$, applying Lemma \ref{one_time_lemma} with  $g\colon\mathbb{R}^d\to\mathbb{R}$ given by[m
[32m+[m[32m\begin{equation*}[m
[32m+[m[32m  g(\cdot,*)=(f(\cdot )+*)-b_n(f,x)[m[41m  [m
[32m+[m[32m\end{equation*},[m[41m [m
[32m+[m[32mwe have $Y_i-b_n(f,x)=g(X_i,\epsilon_i)$. Now using the fact that[m[41m [m
 [m
[32m+[m[32m\begin{equation*}[m
[32m+[m[32m    E[(Y_i-b_n(f,x))a(x,X_i)]=0[m
[32m+[m[32m\end{equation*}[m
[32m+[m[32mgives[m
 \begin{equation}[m
 \label{dr_trick}[m
[31m-\begin{split}[m
[31m-    &E[(Y_i-b_n(f,x))(Y_j-b_n(f,x))a(x,X_i)a(x,X_j)R_i(x)R_j(x)]=\\[m
[31m-    &E[(Y_i-b_n(f,x))(Y_j-b_n(f,x))a(x,X_i)a(x,X_j)R_{i,j}^2(x)]=\\[m
[31m-    &E[(Y_i-b_n(f,x))a(x,X_i)]E[(Y_j-b_n(f,x)a(x,X_j))]ER_{i,j}^2(x)=0[m
[31m-\end{split}[m
[32m+[m[32m    E[(Y_i-b_n(f,x))(Y_j-b_n(f,x))a(x,X_i)a(x,X_j)R_i(x)R_j(x)]=0[m
 \end{equation}[m
[31m-[m
 Furthermore,[m
 \begin{equation}[m
 \label{sr_trick}[m
[36m@@ -401,9 +725,9 @@[m [mFurthermore,[m
 [m
 \end{proof}[m
 \subsection{Lower bounds}[m
[31m-We show that the presence of noise alone is sufficient for a lower bound of $E(\hat{f}_{GNW}(x)-b_n(f,x))^2$ that is of order $\frac{1}{d_n(x)}$.  [m
[31m-\begin{lemma}[m
[31m-\label{variance_lower_bound}[m
[32m+[m[32mWe show that the presence of noise alone is sufficient for a lower bound of $E(\hat{f}_{GNW}(x)-b_n(f,x))^2$ of order $\frac{1}{d_n(x)}$.[m[41m  [m
[32m+[m[32m\begin{lemma}[m[41m [m
[32m+[m[32m  \label{variance_lower_bound}[m
 \begin{equation*}[m
     E(\hat{f}_{GNW}(x)-b_n(f,x))^2\geq \frac{\sigma^2(1-e^{-d_n(x)})^2}{d_n(x)}[m
 \end{equation*}[m
[36m@@ -417,7 +741,7 @@[m [mBy Equation (\ref{variance_decomp}), Lemma[m
 [m
 \begin{equation}[m
 \begin{split}[m
[31m-E(\hat{f}_{GNW}(x)-b_n(f,x))^2&\geq E(\hat{f}_{GNW}(x)-b_n(f,x)I(\sum_{i=1}^n a(x,X_i)>0))^2\\[m
[32m+[m[32mE(\hat{f}_{GNW}(x)-b_n(f,x))^2&\geq E(\hat{f}_{GNW}(x)-b_n(f,x)Z)^2\\[m
 &=n[E(f(X_1)-b_n(f,x))^2a(x,X_i)+\sigma^2c_n(x)]ER_1^2(x)\\[m
 &\geq \sigma^2nc_n(x)ER_1^2(x)\\[m
 &\geq \frac{\sigma^2(1-(1-c_n(x))^n)^2}{nc_n(x)}\\[m
[36m@@ -429,16 +753,15 @@[m [mE(\hat{f}_{GNW}(x)-b_n(f,x))^2&\geq E(\hat{f}_{GNW}(x)-b_n(f,x)I(\sum_{i=1}^n a([m
 [m
 \subsection{Upper bounds}[m
 [m
[32m+[m[32mThe following lemma is crucial towards an upper bound in the variance term (\ref{variance_term}).[m
[32m+[m
 \begin{lemma} [m
 \label{trick_lemma_pt2}[m
[31m-For $n\geq 3$[m
[32m+[m[32mFor $n\geq 3$ and $f$ bounded measurable function with $||f||_{\infty}\leq B$, we have[m
 \begin{equation*}[m
[31m-    E(\hat{f}_{GNW}(x)-b_n(f,x)I(\sum_{i=1}^n a(x,X_i)>0))^2\leq (4B^2+\sigma^2)(\frac{65}{d_n(x)})[m
[32m+[m[32m    E(\hat{f}_{GNW}(x)-b_n(f,x)Z)^2\leq (4B^2+\sigma^2)(\frac{65}{d_n(x)})[m
 \end{equation*}[m
[31m-[m
 \end{lemma}[m
[31m-[m
[31m-[m
 \begin{proof}[m
 Recalling Lemma \ref{trick_lemma_pt_1} and using the fact that $||f||_{\infty}\leq B$, we have [m
 [m
[36m@@ -448,27 +771,22 @@[m [mRecalling Lemma \ref{trick_lemma_pt_1} and using the fact that $||f||_{\infty}\l[m
      n[E(f(X_1)-b_n(f,x))^2a(x,X_i)+\sigma^2c_n(x)]ER_1^2(x)[m
      \leq (4B^2+\sigma^2)nc_n(x)ER^2_1(x)[m
 \end{equation}[m
[31m-[m
[31m-Hence it suffices to control $ER_1^2(x)$. We do this by splitting the expectation on the event that we observe at least $\frac{1}{2}(n-1)c_n(x)$ edges from $a(x,X_i)$, $i=2,...,n$ and on it's complement. Let [m
[32m+[m[32mHence it suffices to control $ER_1^2(x)$. We do this by splitting the expectation on the event that we observe at least $\frac{1}{2}(n-1)c_n(x)$ edges from $a(x,X_i)$, $i=2,...,n$ and on its complement. In the event that we observe at least $\frac{1}{2}(n-1)c_n(x)$ edges $R_1$ will be bounded from above by a quantity of order $\frac{C}{d_n(x)}$, for an explicit constant $C>0$. The main observation is that $R_1\leq 1$ and observing too few edges is an event with small probability. The rest of the proof deals with technical calculations. Let[m[41m [m
 \begin{equation}[m
     A(x)=\{\sum_{i=2}^na(x,X_i)\geq \frac{1}{2}(n-1)c_n(x)\}[m
 \end{equation}[m
[31m-[m
 For $n\geq 2$ we have[m
 [m
 \begin{equation}[m
 \label{meat_good_part}[m
 ER_1^2(x)I(A(x))\leq \frac{1}{(1+\frac{1}{2}(n-1)c_n(x))^2}P(A(x))\leq \frac{16}{n^2c_n^2(x)}[m
 \end{equation}[m
[31m-[m
 An application of Bernstein's inequality for bounded distributions (\cite{vershynin} Theorem 2.8.4, page 39) with $a(x,X_i)-c_n(x)$, $i=2,3,...n$ as the bounded, centered and independent variables yields[m
[31m-[m
 \begin{equation}[m
 \label{bernstein_result}[m
 P(|\sum_{i=2}^{n}a(x,X_i)-(n-1)c_n(x)|\geq t)\leq 2\exp{(-\frac{t^2}{(n-1)c_n(x)(1-c_n(x))+\frac{t}{3}})}[m
 \end{equation}[m
 Setting $t=\frac{1}{2}(n-1)c_n(x)$ in Equation (\ref{bernstein_result}) together with the observation that $A^c(x)$ implies [m
[31m-[m
 \begin{equation*}[m
     |\sum_{i=2}^{n}(a(x,X_i)-c_n(x))|\geq \frac{1}{2}(n-1)c_n(x)[m
 \end{equation*}[m
[36m@@ -484,28 +802,30 @@[m [mFor $n\geq 3$, we get[m
 \end{split}[m
 \end{equation}[m
 Using the fact that $R_1\leq 1$ along with Equation (\ref{meat_bad_part_anticip}) we get[m
[31m-[m
 \begin{equation}[m
 \label{meat_bad_part}[m
     ER^2_1(x)I(A^c(x))\leq P(A^c(x))\leq \exp(-\frac{nc_n(x)}{7})[m
 \end{equation}[m
[31m-[m
 Combining Equation (\ref{meat_good_part}) and Equation (\ref{meat_bad_part}) gives[m
 [m
 \begin{equation}[m
 \label{meat}[m
 ER^2_1(x)\leq \frac{16}{n^2c_n^2(x)}+\exp(-\frac{nc_n(x)}{7})[m
 \end{equation}[m
[32m+[m[32mFinally,[m
[32m+[m[32m\begin{equation}[m
[32m+[m[32m    d_n(x)ER^2_1(x)\leq \frac{16}{d_n(x)}+d_n(x)\exp(-\frac{d_n(x)}{7})[m
[32m+[m[32m\end{equation}[m
 The conclusion follows by combining Equationms (\ref{ubv_1}) and  (\ref{meat}), together with the basic inequality which states that for all $x\geq 0$, $x^2e^{-x}\leq 1$.[m
 \end{proof}[m
 [m
[31m-\begin{lemma}[m
[32m+[m[32m\begin{theorem}[m
 \label{variance_lemma}[m
 [m
 \begin{equation*}[m
     E(\hat{f}_{GNW}(x)-b_n(f,x))^2\leq\frac{261B^2+65\sigma^2}{d_n(x)}[m
 \end{equation*}[m
[31m-\end{lemma}[m
[32m+[m[32m\end{theorem}[m
 [m
 \begin{proof}[m
 [m
[36m@@ -515,11 +835,12 @@[m [mLemma \ref{trick_lemma_pt2} and using the basic inequality $1-t\leq \exp{(-t)}$[m
 \begin{equation*}[m
 \begin{split}[m
     E(\hat{f}_{GNW}(x)-b_n(f,x))^2[m
[31m-    &=E(\hat{f}_{GNW}(x)-I(\sum_{i=1}^na(x,X_i)b_n(f,x))^2+b_n^2(f,x)P(\sum_{i=1}^n a(x,X_i)=0)\\[m
[32m+[m[32m    &=E(\hat{f}_{GNW}(x)-b_n(f,x)Z)^2+b_n^2(f,x)P(\sum_{i=1}^n a(x,X_i)=0)\\[m
     &\leq (4B^2+\sigma^2)(\frac{16}{nc_n(x)}+nc_n(x)\exp{(-\frac{nc_n(x)}{7})})+B^2(1-c_n(x))^n\\[m
     &\leq (4B^2+\sigma^2)(\frac{16}{nc_n(x)}+nc_n(x)\exp{(-\frac{nc_n(x)}{7})})+B^2\exp(-nc_n(x))[m
 \end{split}[m
 \end{equation*}[m
[32m+[m[32mNote that this result is slightly stronger than the statement of the theorem. For simplicity, we bound every term by the dominating term $\frac{1}{d_n(x)}$.[m
 We conclude by using the basic inequalities:[m
 for all $t\geq 0$, $t^2e^{-t}\leq 1$ and $te^{-t}\leq 1$.[m
 \end{proof}[m
[36m@@ -529,7 +850,7 @@[m [mSuppose that $d_n(x)\to\infty$ as $n\to\infty$. Then Lemma (\ref{variance_lemma}[m
 \begin{equation*}[m
     E(\hat{f}_{GNW}(x)-b_n(f,x))^2\to 0[m
 \end{equation*}[m
[31m-The growth of $d_n(x)$ can be arbitrarily slow, and the statement still holds. On the other hand if $d_n(x)\leq D$ for all $n\in\mathbb{N}$ and $\sigma^2>0$ then Lemma \ref{variance_lower_bound} gives [m
[32m+[m[32mOn the other hand if $d_n(x)\leq D$ for all $n\in\mathbb{N}$ and $\sigma^2>0$ then Lemma \ref{variance_lower_bound} gives[m[41m [m
 [m
 \begin{equation*}[m
     E(\hat{f}_{GNW}(x)-b_n(f,x))^2\geq \frac{\sigma^2(1-e^{-D})}{D}>0[m
[36m@@ -537,63 +858,101 @@[m [mThe growth of $d_n(x)$ can be arbitrarily slow, and the statement still holds. O[m
 [m
 \end{remark}[m
 [m
[31m-\section{Controlling the Bias at a point}[m
[31m-\subsection{Geometric concerns}[m
 [m
[31m-\paragraph{Disconnected LPMs}[m
[32m+[m[32m\section{Uniform control of the bias}[m
[32m+[m[32m\label{uniform_bias_control}[m
[32m+[m[32mUp until this point the set we considered points $x\in\mathbb{R}^d$ such that[m
[32m+[m[32mfor all $n\in\mathbb{N}$, $c_n(x)>0$ and we have established the fact that[m
[32m+[m[32msignal averaging estimator on graphs in the Latent Position Model concentrates[m
[32m+[m[32mtowards a quantity $b_n(f,x)$. Now we address the question: Under which[m
[32m+[m[32mconditions is $b_n(f,x)$ a good approximation of $f(x)$?[m
[32m+[m
[32m+[m[32m\SB{I really don't understand the point you are trying to make in the next[m
[32m+[m[32m  paragraph. }[m
[32m+[m
[32m+[m[32mSuppose that $X$ is a random variable with density $p$. Then for any $x\in\mathbb{R}^d$, we have[m
[32m+[m[32m\begin{equation*}[m
[32m+[m[32m    P(|X-x|<r)=\int_{B_r(x)}p(z)dz[m
[32m+[m[32m\end{equation*}[m
[32m+[m[32mConsequently, if there exists $r>0$ such that $\int_{B_r(x)}p(z)dz=0$, then[m
[32m+[m[32m$P(|X-x|\geq r)=1$.[m
[32m+[m
[32m+[m[32mDue to out geometrical[m[41m [m
[32m+[m[32m--asymptotically decreasing bandwith - such points will be ignored[m[41m [m
[32m+[m
[32m+[m
[32m+[m[32m--to motivate the definition of support, you need to explain bandwith adaptation first[m
 [m
[31m-Since we want to consider the case where $X$ is a random variable, we need to make sure that $b_n(f,X)$ is well defined. In a general Latent position model it could happen that with positive probability for all $n\in\mathbb{N}$, $c_n(X)=0$, which means that with positive probability $X$ will be an isolated node in the graph. To avoid such trivialities [m
 [m
[31m-\paragraph{Regularity}[m
[32m+[m[32m--[m
 [m
 [m
 [m
[31m-\paragraph{Kernel assumptions}[m
[31m-These assumptions on $k_n$ in simple terms say that edge generation is dependent only on distances and thus make the model comparable to the Random Geometric Graph. It is an assumption so strong that it implies that  $dp(x)$-almost surely  $b_n(f,x)\rightarrow f(x)$ for a very general class of functions $f$. However, pointwise convergence of this form without uniform control of the error $|b_n(f,x)-f(x)|$ is not useful for our purposes. The following assumptions on $f$  [m
[31m-provide an easy way for bound on $\sup_{x\in\supp{p}}|b_n(f,x)-f(x)|$.[m
[31m-Finally, in order to translate our results for fixed points $x\in\supp{p}$ into results for a random variable $X$ with distribution $p$, we will need an assumption on the density $p$ itself.[m
[32m+[m[32mA random variable $X$ with density $p$ takes values in $\supp{p}$. In anticipation of results to come, from now on we only consider points $x\in\supp{p}$. The goal of this section is to prove a bound of the bias term (\ref{bias_term}) which is uniform over $x\in\supp{p}$. We will need some assumptions on the kernel and the regularity of the function $f$.[m[41m [m
 [m
[31m-\paragraph{Density assumptions}[m
[32m+[m[32m\subsection{Kernel assumptions} We will assume that[m
[32m+[m[32m\begin{equation}[m
[32m+[m[32m\label{kernel_def}[m
[32m+[m[32mk_n(x,z)=K(\frac{x-z}{h_n})[m[41m    [m
[32m+[m[32m\end{equation}[m
[32m+[m[32mwhere $K\colon\mathbb{R}^d\to [0,1]$ and $h_n>0$, $h_n\to 0$ as $n\to\infty$. We assume that $K$ satisfies the following conditions[m[41m [m
 \begin{itemize}[m
[31m-    \item $p\colon\mathbb{R}^d\to\mathbb{R}$ is $\beta$-Holder continous on it's support $\supp{p}$ i.e.[m
[31m-    there exists $L_{\beta}>0$ such that [m
[31m-    [m
[32m+[m[32m    \item There exists $M_1>0$ such that for all[m
[32m+[m[32m    $z\in\mathbb{R}^d$[m[41m [m
     \begin{equation*}[m
[31m-        \sup_{x,z\in\supp{p}} \frac{|p(x)-p(z)|}{||x-z||^{\beta}}\leq L_{\beta}  [m
[32m+[m[32m        \frac{1}{2}I(||z||\leq M_1)\leq K(z)[m
[32m+[m[32m    \end{equation*}[m
[32m+[m[32m    \item There exists $M_2>0$ such that for all[m
[32m+[m[32m    $z\in\mathbb{R}^d$[m
[32m+[m[32m    \begin{equation*}[m
[32m+[m[32m        K(z)\leq I(||z||\leq M_2)[m
     \end{equation*}[m
[31m-    [m
[31m-    [m
 \end{itemize}[m
[32m+[m[32mThese assumptions are a generalization of the Random Geometric Graph. The assumption $K1$ is relatively weak, and it will be important in understanding how the expected degree at $x$ \ref{local_degree} relates to $h_n$. The assumption $K2$ is much stronger, but it will be important for controlling the bias term (\ref{bias_term}).[m
 [m
[31m-\begin{lemma}[m
[31m-\label{well_cond_lemma}[m
[31m-Suppose that \textbf{Kernel assumptions} hold. Then [m
[31m-[m
[31m-\begin{equation*}[m
[31m-    \supp{p}\subseteq \{x\in\mathbb{R}^d:c_n(x)>0\}[m
[32m+[m[32m\subsection{Function Assumptions} We assume that the function $f\colon\mathbb{R}^d\to\mathbb{R}$ satisfies the following conditions[m
[32m+[m[32m\begin{itemize}[m
[32m+[m[32m    \item There exists $B>0$ such that[m[41m [m
[32m+[m[32m    \begin{equation*}[m
[32m+[m[32m        \sup_{x\in\supp{p}} |f(x)| \leq B[m
[32m+[m[32m    \end{equation*}[m
[32m+[m[32m    \item There exists $0<\alpha\leq 1$ and $L>0$ such that[m
[32m+[m[41m    [m
[32m+[m[32m    \begin{equation*}[m
[32m+[m[32m        \sup_{x,z\in\supp{p},x\neq z} \frac{|f(x)-f(z)|}{||x-z||^{\alpha}}\leq L[m
[32m+[m[32m    \end{equation*}[m
[32m+[m[32m\end{itemize}[m
[32m+[m
[32m+[m[32m\subsection{Uniform bound}[m
[32m+[m
[32m+[m[32m\begin{lemma}[m
[32m+[m[32m\label{well_cond_lemma}[m
[32m+[m[32mSuppose that \textbf{Kernel assumptions} hold. Then[m
[32m+[m[32m\begin{equation*}[m
[32m+[m[32m    \supp{p}\subseteq \{x\in\mathbb{R}^d:c_n(x)>0\}[m
 \end{equation*}[m
[31m-\begin{proof}[m
[31m-Suppose that $c_n(x)=0$. By continuity of $K$ at $0$, there is $r>0$ such that for all $z$ for which $||x-z||\leq rh_n$, $K(\frac{x-z}{h_n})\geq \frac{1}{2}$. Hence  [m
 [m
[32m+[m[32m\end{lemma}[m
[32m+[m[32m\begin{proof}[m
[32m+[m[32mSuppose that $c_n(x)=0$.[m
[32m+[m[32mUsing the assumption that $\frac{1}{2}I(|x-z||\leq M_1h_n)\leq k_n(x,z)$ we get[m[41m [m
 \begin{equation*}[m
 \begin{split}[m
[31m-    \int I(||x-z||\leq rh_n) p(z)dz &\leq 2\int I(||x-z||\leq rh_n)K(\frac{x-z}{h_n})p(z)dz\\[m
[32m+[m[32m    \int I(||x-z||\leq M_1h_n) p(z)dz &\leq 2\int I(||x-z||\leq rh_n)K(\frac{x-z}{h_n})p(z)dz\\[m
     &\leq 2\int K(\frac{x-z}{h_n})p(z)dz\\[m
     &=2c_n(x)\\[m
     &=0[m
 \end{split}[m
 \end{equation*}[m
 Hence $x\notin\supp{p}$, this proves the claim by contraposition.[m
[31m-[m
 \end{proof}[m
 [m
 [m
[31m-\end{lemma}[m
 [m
 \begin{lemma}[m
 \label{bias_control_lemma}[m
 [m
[31m-Suppose that \textbf{Kerenl assumptions} and \textbf{Function assumptions} hold. Then[m
[32m+[m[32mSuppose that \textbf{Kernel assumptions} and \textbf{Function assumptions} hold. Then[m
     \begin{equation*}[m
     \sup_{x\in \supp(p)}|b_n(f,x)-f(x)|\leq L_{\alpha}M^{\alpha}h_n^{\alpha}[m
 \end{equation*}[m
[36m@@ -605,7 +964,7 @@[m [mWe have[m
 [m
 \begin{equation*}[m
 \begin{split}[m
[31m-|b_n(f,x)-f(x)|&=|\frac{\rho_n\int f(z)K(\frac{x-z}{h_n})p(z)dz}{\rho_n\int K(\frac{x-z}{h_n})p(z)dz}-f(x)|\\[m
[32m+[m[32m|b_n(f,x)-f(x)|&=|\frac{\int f(z)K(\frac{x-z}{h_n})p(z)dz}{\int K(\frac{x-z}{h_n})p(z)dz}-f(x)|\\[m
 &=|\frac{\int f(z)K(\frac{x-z}{h_n})p(z)dz}{\int K(\frac{x-z}{h_n})p(z)dz}-\frac{\int f(x)K(\frac{x-z}{h_n})p(z)dz}{\int K(\frac{x-z}{h_n})p(z)dz}|\\[m
 &=|\frac{\int[f(z)-f(x)]K(\frac{x-z}{h_n})p(z)dz[m
 }{\int K(\frac{x-z}{h_n})p(z)dz}|\\[m
[36m@@ -615,87 +974,229 @@[m [mWe have[m
 &\leq L_{\alpha}M^{\alpha}h_n^{\alpha}[m
 \end{split}[m
 \end{equation*}[m
[31m-Here, we used the fact that for any function $G\in L^1(dp(x))$, $\int_{\mathbb{R}^d} G(z)p(z)dz=\int_{\supp{p}} G(z)p(z)dz$ and crucially the facts that $f$ is $\alpha$-Holder continuous and that $K$ is compactly supported in the last inequality.[m
[32m+[m[32mHere, we used the fact that for any function $G\in L^1(dp(x))$, $\int_{\mathbb{R}^d} G(z)p(z)dz=\int_{\supp{p}} G(z)p(z)dz$ and crucially the facts that $f$ is $\alpha$-H√∂lder continuous and that $K$ is compactly supported in the last inequality.[m
 \end{proof}[m
 [m
[31m-\begin{corollary} Suppose that \textbf{Kernel assumptions} and \textbf{Function assumptions} hold. Let $X$ be a random variable with density $p$. [m
[31m-\begin{equation*} [m
[31m-P(|b_n(f,X)-f(X)|\leq L_{\alpha}M^{\alpha}h_n^{\alpha})=1[m
[32m+[m
[32m+[m[32m\section{Risk convergence}[m
[32m+[m
[32m+[m[32m\subsection{Risk convergence at a point}[m
[32m+[m[32mFor a given $x\in\supp{p}$ we want to control the variance term (\ref{variance_term}) and the bias term (\ref{bias_term}) at the same time. The bias term is already uniformly bounded over $\supp{p}$ by Lemma (\ref{bias_control_lemma}). By Lemma (\ref{variance_lemma}) it suffices to bound $\frac{1}{c_n(x)}$. Up to this point the result were not dependent on the density $p$. Observe that under \textbf{Kernel assumptions} we have that for all $x\in\supp{p}$[m[41m [m
[32m+[m
[32m+[m[32m\begin{equation*}[m
[32m+[m[32m    c_n(x)=\int K(\frac{x-z}{h_n})p(z)dz\geq \ \int I(|x-z|\leq M_1h_n)p(z)dz[m
 \end{equation*}[m
[32m+[m[32mBefore we make stronger assumptions on the density $p$, we show that the kernel assumptions and function assumptions are strong enough to guarantee that as soon as $nh_n^d\to\infty$ as $n\to\infty$, the variance term \ref{variance_term} converges to zero for dp- almost every $x\in\mathbb{R}^d$.[m
[32m+[m
[32m+[m[32m\begin{lemma}[m
[32m+[m[32m\label{Lebesgue_lemma}[m
[32m+[m[32mSuppose that \textbf{Kernel assumptions} hold. Let $v_d$ be the Lebesgue measure of the unit ball in $\mathbb{R}^d$. Then almost everywhere in the sense of Lebesgue measure,[m
[32m+[m
[32m+[m[32m\begin{equation*}[m
[32m+[m[32m \frac{1}{2}v_dM_1^dp(x)\leq \liminf_{n\to\infty} \frac{c_n(x)}{h_n^d}\leq \limsup_{n\to\infty} \frac{c_n(x)}{h_n^d}\leq v_dM_2^dp(x)[m[41m   [m
[32m+[m[32m\end{equation*}[m
[32m+[m
[32m+[m[32m\end{lemma}[m
 [m
[31m-\end{corollary}[m
 \begin{proof}[m
[31m-We have[m
[32m+[m[32mUsing the \textbf{Kernel assumptions} we have[m
[32m+[m
[32m+[m[32m\begin{equation}[m
[32m+[m[32m\label{lebesgue_density_pts}[m
[32m+[m[32m    \frac{1}{2}\int_{|x-z|\leq M_1h_n}p(z)dz\leq c_n(x)\leq \int_{|x-z|\leq M_2h_n}p(z)dz[m
[32m+[m[32m\end{equation}[m
[32m+[m[32mBy Lebesgue's differentiation theorem,[m[41m [m
[32m+[m
[32m+[m[32m\begin{equation}[m
[32m+[m[32m\frac{\int_{|x-z|\leq M_1h_n}p(z)dz}{h_n^d}\to\ v_dM_1^p(x)[m
[32m+[m[32m\end{equation}[m
[32m+[m[32mand[m
[32m+[m[32m\begin{equation}[m
[32m+[m[32m\frac{\int_{|x-z|\leq M_2h_n}p(z)dz}{h_n^d}\to v_dM_2^dp(x)[m
[32m+[m[32m\end{equation}[m
[32m+[m[32mDividing the inequality (\ref{lebesgue_density_pts}) by $h_n^d$ and letting $n\to\infty$ we get the desired result.[m
[32m+[m
[32m+[m[32m\end{proof}[m
[32m+[m
[32m+[m[32m\begin{lemma}[m
[32m+[m[32m\label{pointwise_as_conv}[m
[32m+[m[32mSuppose that \textbf{Kernel assumptions hold} and $h_n^d\to 0$ as $n\to\infty$. Then for dp-almost every $x\in\mathbb{R}^d$ we have[m
 [m
 \begin{equation*}[m
[31m-\begin{split}[m
[31m-    P(|b_n(f,X)-f(X)|\leq L_{\alpha}M^{\alpha}h_n^{\alpha})&=\int I(|b_n(f,x)-f(x)|\leq L_{\alpha}M^{\alpha}h_n^{\alpha})p(x)dx\\[m
[31m-    &\int_{\supp{p}} I(|b_n(f,x)-f(x)|\leq L_{\alpha}M^{\alpha}h_n^{\alpha})p(x)dx\\[m
[31m-    &=1[m
[31m-\end{split}[m
[32m+[m[32m    E(\hat{f}_{GNW}(x)-b_n(f,x))^2\to 0[m
 \end{equation*}[m
[32m+[m[32mas $n\to\infty$.[m
 [m
[31m-where we used Lemma \ref{bias_control_lemma} in the last line.[m
[32m+[m[32m\end{lemma}[m
[32m+[m[32m\begin{proof}[m
[32m+[m[32mThe set $\{x\in\supp{p}|p(x)=0\}$ has measure $0$ with respect to the measure $dp(x)=p(x)dx$. Pick a point $x\in\supp{p}$ which is also a Lebesgue point for $p$. This is a set of full measure with respect to $dp$. By Lemma \ref{Lebesgue_lemma} for all Lebesgue points of $p$ for which $p(x)>0$ we have that there exists $n(x)\in\mathbb{N}$ such that for $n\geq n(x)$[m
[32m+[m
[32m+[m[32m\begin{equation*}[m
[32m+[m[32m    \frac{1}{d_n(x)}=\frac{1}{nc_n(x)}\leq {1}{4nh_n^d}[m
[32m+[m[32m\end{equation*}[m
[32m+[m[32mWe conclude by recalling Lemma \ref{variance_lemma}.[m
[32m+[m[32m\end{proof}[m
[32m+[m
[32m+[m[32m\begin{corollary}[m
[32m+[m[32m\label{useless_corollary}[m
[32m+[m[32mSuppose that Kernel assumptions and Function assumptions hold, $nh_n^d\to 0$ and $h_n\to 0$.[m
[32m+[m[32mThen for dp-almost every $x\in\mathbb{R}^d$,[m
 [m
[32m+[m[32m\begin{equation*}[m
[32m+[m[32m    E(\hat{f}_{GNW}(x)-f(x))^2\to 0[m
[32m+[m[32m\end{equation*}[m
[32m+[m[32mas $n\to\infty$[m
[32m+[m[32m\end{corollary}[m
[32m+[m[32m\begin{proof}[m
[32m+[m[32mThe statement follows directly from Lemma \ref{bias_control_lemma} and Lemma \ref{pointwise_as_conv}[m[41m [m
 \end{proof}[m
[32m+[m[32mCorollary \ref{useless_corollary} gives a very weak form of convergence which is not very informative about the rates (or even the points where convergence occurs). To prove stronger results we need stronger assumptions. Lemma \ref{local_variance_lemma} provides a local condition under which  that variance term \ref{variance_term} will vanish.[m
 [m
[31m-Lemma \ref{density_lemma} shows that under \textbf{Density assumptions} the .[m
 [m
 \begin{lemma}[m
[31m-\label{density_lemma}[m
[31m-\end{lemma}[m
[32m+[m[32m\label{local_variance_lemma} Suppose that \textbf{Kernel assumptions} hold and that $x\in\supp{p}$ is such that there exists $r>0$ with[m
[32m+[m[32m\begin{equation*}[m
[32m+[m[32m    \inf_{|x-z|\leq r} p(z)\geq p_0>0[m
[32m+[m[32m\end{equation*}[m
[32m+[m[32mIf $M_1h_n<r$ then[m
 [m
[31m-\section{Risk convergence for a fixed point}[m
[32m+[m[32m\begin{equation*}[m
[32m+[m[32m    E(\hat{f}_{GNW}(x)-b_n(f,x))^2\leq \frac{65(4B^2+\sigma^2)}{p_0v_dM_1^dnh_n^d}[m
[32m+[m[32m\end{equation*}[m
[32m+[m[32mIn particular, if $nh_n^d\to\infty$ as $n\to\infty$ then as $n\to\infty$[m
 [m
 [m
[31m-\subsection{Main Results}[m
[32m+[m[32m\begin{equation*}[m
[32m+[m[32mE(\hat{f}_{GNW}(x)-b_n(f,x))^2\to 0[m[41m    [m
[32m+[m[32m\end{equation*}[m
 [m
[32m+[m[32m\end{lemma}[m
[32m+[m[32m\begin{proof}[m
[32m+[m[32mNote that when $M_1h_n<r$ the assumption on $p$ implies[m
 [m
[32m+[m[32m\begin{equation*}[m
[32m+[m[32mc_n(x)\geq \rho_n\int I(|x-z|\leq M_1h_n)p(z)dz\geq p_0v_dM_1^dh_n^d[m
[32m+[m[32m\end{equation*}[m
[32m+[m[32mHence[m
 [m
[31m-\begin{theorem} Assume that \textbf{Kernel assumptions} and \textbf{Function assumptions} hold.[m
[32m+[m[32m\begin{equation*}[m
[32m+[m[32m    \frac{1}{d_n(x)}=\frac{1}{nc_n(x)}\leq \frac{1}{p_0v_dM_1^dnh_n^d}[m
[32m+[m[32m\end{equation*}[m
[32m+[m[32mWe conclude by applying Lemma \ref{variance_lemma}.[m
[32m+[m[32m\end{proof}[m
[32m+[m[32m\subsection{Density assumptions}[m
[32m+[m[32mAssuming that on its support $p$ is bounded from below by a positive constant\footnote{This assumption implies that $\supp{p}$ is compact} is the simplest way to obtain lower bound for $c_n(x)$. In addition the geometry of $\supp{p}$ also plays an important role.[m
 [m
[31m-If $mh_n\leq d(x,\partial\supp{p}) $[m
[32m+[m[32mWe suppose that[m[41m [m
[32m+[m[32m\begin{itemize}[m
[32m+[m[32m    \item $\supp{p}=Q_d=[-1,1]^d$[m
[32m+[m[32m    \item For all $x\in[-1,1]^d$[m[41m [m
[32m+[m[32m    \begin{equation*}[m
[32m+[m[32m        p(x)\geq p_0>0[m
[32m+[m[32m    \end{equation*}[m
[32m+[m[32m\end{itemize}[m
[32m+[m[32mWe will work with the cube $Q_d$ for simplicity. The main geometric property that $Q_d$ satisfies is that for sufficiently small radii, $Q_d$ retains a portion of the Lebesgue measure of any ball $B_r(x)$ ball centered at $x\in Q_d$.[m
[32m+[m
[32m+[m[32m\begin{lemma}[m
[32m+[m[32m\label{measure_retention_lemma}[m
[32m+[m[32mFor all $x\in Q_d$ and all $r\leq 1$[m[41m [m
[32m+[m[32m\begin{equation}[m
[32m+[m[32m    m(Q_d\cap B_r(x))\geq \frac{1}{2^d}m(B_r(x))[m
[32m+[m[32m\end{equation}[m
[32m+[m[32m\end{lemma}[m
[32m+[m
[32m+[m[32m\subsection{Risk of a random point}[m
 [m
[32m+[m[32m\begin{lemma}[m
[32m+[m[32m\label{risk_at_random_point}[m
[32m+[m[32mAssume that \textbf{Kernel assumptions}, \textbf{Function assumptions} and[m
[32m+[m[32m\textbf{Density assumptions} hold. If $X$ is a random variable with density $p$ independent from all variables appearing in $\hat{f}_{GNW}$, then[m[41m [m
 [m
 \begin{equation*}[m
[31m-    E(\hat{f}_{GNW}(x)-f(x))^2\leq 2[m
[32m+[m[32m    E(\hat{f}_{GNW}(X)-b_n(f,X))^2\leq \frac{260B^2+65\sigma^2}{p_0(M_1/2)^dv_dnh_n^d}[m
 \end{equation*}[m
[32m+[m[32m\end{lemma}[m
 [m
[31m-\end{theorem}[m
[32m+[m[32m\begin{proof}[m
[32m+[m[32mUsing \textbf{Density assumptions}, \textbf{kernel assumptions} we have that for all $x\in Q_d$[m
[32m+[m
[32m+[m[32m\begin{equation}[m
[32m+[m[32m    c_n(x)\geq \int_{|x-z|\leq M_1h_n}p(z)dz\geq p_0 \int_{Q_d} I(|x-z|\leq M_1h_n)dz[m
[32m+[m[32m\end{equation}[m
[32m+[m[32mUsing Lemma \ref{measure_retention_lemma}, if $M_1h_n\leq 1$ we have[m
 [m
[31m-\subsection{Discussion}[m
[32m+[m[32m\begin{equation*}[m
[32m+[m[32m    c_n(x)\geq \frac{v_d}{2^d}p_0M_1^dh_n^d[m
[32m+[m[32m\end{equation*}[m
[32m+[m[32mWe have[m[41m [m
[32m+[m[32m\begin{equation*}[m
[32m+[m[32m    \frac{1}{d_n(x)}=\frac{1}{nc_n(x)}\leq \frac{1}{p_0(M_1/2)^dv_dnh_n^d}[m
[32m+[m[32m\end{equation*}[m
[32m+[m[32mFinally, we have[m
[32m+[m[32m\begin{equation}[m
[32m+[m[32m\begin{split}[m
[32m+[m[32m    E(\hat{f}_{GNW}(X)-b_n(f,X))^2&=\int E(\hat{f}_{GNW}(x)-b_n(f,x))^2p(x)dx\\[m
[32m+[m[32m    &\leq \int \frac{p(x)dx}{d_n(x)}\\[m
[32m+[m[32m    &\leq \int \frac{65(4B^2+\sigma^2)p(x)dx}{p_0M_1^dv_dnh_n^d}\\[m
[32m+[m[32m    &=\frac{65(4B^2+\sigma^2)}{p_0(M_1/2)^dv_dnh_n^d}[m
[32m+[m[32m\end{split}[m
[32m+[m[32m\end{equation}[m
[32m+[m[32m\end{proof}[m
 [m
[31m-\paragraph{Remark 7 (The effect of $\lambda_n$)} As mentioned in the proof of Lemma 3, $\lambda_n$ has no effect on the bias term. However, if we want to keep the consistency properties of $\hat{f}_{GNW}$ via Lemma 4, we see that shrinking $\lambda_n$ forces $h_n$ to increase, and as can be seen from Lemma 3 this loosens the bound on the bias term. In this sense the assumption $\lambda_n\geq [m
[31m-\lambda>0$ is optimal for convergence properties of $\hat{f}_{GNW}$.[m
[32m+[m[32m\begin{theorem}[m
[32m+[m[32m\label{final_result}[m
[32m+[m[32mSuppose that \textbf{Kernel assumptions}, \textbf{Function assumptions} and \textbf{Density assumptions} hold. Then for a random variable $X$ independent from all variables appearing in $\hat{f}_{GNW}$, we have[m
 [m
[31m-\paragraph{Remark 8 (Bias-variance tradeoff)}[m
[31m-For the sake of simplicity, we suppose that $\lambda_n=1$. Then Theorem 1 states that   $P(|\hat{f}_{GNW}(x)-\frac{T_{k_n}(f)(x)}{T_{k_n}(1)(x)}|\geq \delta)[m
[31m-\leq c_1\exp(-c\delta^2h_n^{2d}n) $. Since we want this probability to be small we need to have $h_n^dn\rightarrow\infty$. In fact the for the purpose of low variance, large values of $h_n$ are good. However, for the purpose of low bias, as per Lemma 3, small values of $h_n$ are preferred. In particular, we see that if $h_n=\omega(\frac{1}{n^{1/2d}})$ and $h_n=o(1)$ then a good bias-variance tradeoff has been achieved and consistency properties of $\hat{f}_{GNW}$ follow.[m
[32m+[m[32m\begin{equation*}[m
[32m+[m[32m    E(\hat{f}_{GNW}(X)-f(X))^2\leq \frac{130(4B^2+\sigma^2)}{p_0(M_1/2)^dv_dnh_n^d}+2L^2_{\alpha}M_2^{2\alpha}h_n^{2\alpha}[m
[32m+[m[32m\end{equation*}[m
[32m+[m[32m\end{theorem}[m
[32m+[m[32m\begin{proof}[m
[32m+[m[32mThe result immediately follows from the basic inequality $(a+b)^2\leq 2(a^2+b^2)$, Lemma \ref{bias_control_lemma} and Lemma  \ref{risk_at_random_point}.[m
[32m+[m[32m\end{proof}[m
[32m+[m[32mApplying Chebyshev's inequality to Corollary \ref{final_result} we get a confidence interval[m[41m [m
[32m+[m[32m\begin{corollary}[m
[32m+[m[32m\label{final_result_concentration}[m
[32m+[m[32mSuppose that \textbf{Kernel assumptions}, \textbf{Function assumptions} and \textbf{Density assumptions} hold. Then for a random variable $X$ with density $p$ independent from all variables appearing in $\hat{f}_{GNW}$, we have[m
 [m
[31m-\paragraph{Remark 9 (Curse of dimensionality)}[m
[31m-We observe the well known phenomenon known as the curse of dimensionality, which states that sample complexities grow exponentially in the dimension of the data.[m
[32m+[m[32m\begin{equation*}[m
[32m+[m[32m    P(|\hat{f}_{GNW}(X)-f(X)|\geq \delta)\leq \frac{130(4B^2+\sigma^2)}{\delta^2p_0(M_1/2)^dv_dnh_n^d}+\frac{2L_{\alpha}^2M_2^{2\alpha}h_n^{2\alpha}}{\delta^2}[m
[32m+[m[32m\end{equation*}[m
 [m
 [m
[31m-\paragraph{Remark 10 (Non compact case)}[m
[31m-If $\int ||y||^2k(y)dy<\infty$,[m
[31m-    $p$ is $\beta$ Holder continuous, with $0<\beta\leq 1$ and $p(x)>0$ then there exists $n(x)\in \mathbb{N}$ such that for all $n\geq  n(x)$,[m
[31m-    \begin{equation*}[m
[31m-        |\frac{T_{k_n}(f)(x)}{T_{k_n}(1)(x)}-f(x)|\leq ch_n^{\alpha}[m
[31m-    \end{equation*}[m
[31m-    with $c>0$ an absolute constant depending on $k$ and the Holder constants of $f$ and $p$.[m
[32m+[m[32m\end{corollary}[m
 [m
 [m
 [m
[32m+[m[32m\subsection{Bias-variance trade off}[m
[32m+[m[32mFrom  Corollary \ref{final_result} it follows that if $h_n^dn\to\infty$ and $h_n\to 0$ then the estimator $\hat{f}_{GNW}$ is consistent in the sense of \ref{random_point_risk}. Note that as $h_n$ decreases, the bias term \ref{bias_term} decreases, but the bounds on the variance term \ref{variance_term} deteriorate. Conversely, as $h_n$ increases, the variance term \ref{variance_term} decreases but the bounds on the bias term \ref{bias_term} deteriorate. This is also the case with the classical Nadaraya Watson estimator and is a general phenomenon in statistics known as the bias-variance trade off.[m[41m [m
[32m+[m[32m\subsection{Curse of dimensionality}[m
[32m+[m[32mWe can derive a sample complexity in the following way. We look for $n$ large enough such that both terms on the right hand side of the inequality in Corollary \ref{final_result_concentration} are less than $\epsilon/2$.[m
[32m+[m[32mThus we want[m
 [m
[32m+[m[32m\begin{equation}[m
[32m+[m[32m\label{sample_complexity_bias}[m
[32m+[m[32m    h_n^{2\alpha}\leq \frac{\epsilon\delta^2}{L_{\alpha}^2M_2^{2\alpha}}[m
[32m+[m[32m\end{equation}[m
[32m+[m[32mand[m
[32m+[m[32m\begin{equation}[m
[32m+[m[32m\label{sample_complexity_variance}[m
[32m+[m[32m\frac{260(4B^2+\sigma^2)}{\epsilon\delta^2p_0(M_1/2)^dv_dh^d_n}\leq n[m
[32m+[m[32m\end{equation}[m
[32m+[m[32mCombining Equation \ref{sample_complexity_bias} and Equation \ref{sample_complexity_variance} we get[m
[32m+[m
[32m+[m[32m\begin{equation}[m
[32m+[m[32m    n\geq \frac{260(4B^2+\sigma^2)L_{\alpha}^{2d/{\alpha}}}M_2^d{\epsilon\delta^2p_0(M_1/2)^dv_d}[m
[32m+[m[32m\end{equation}[m
 [m
[32m+[m[32mWe observe the well know[m
[32m+[m[32mphenomenon known as the curse of dimensionality, which states that sample complexities grow exponentially in the dimension of the data.[m
 [m
 [m
 [m
 \section{Simulations}[m
 [m
 We test empirically the performance of $\hat{f}_{GNW}$. We assume that the latent data $X_1,...,X_n$ is i.i.d. uniform on $[0,1]^d$ and we compare $\hat{f}_{GNW}(x)$, $\hat{f}_{NW}(x)$, $T_k(f)(x)$ and $f(x)$.[m
[31m-We will study by simulations how the sample size, the dimension of the data and the noise level affects the estimator. We will also study how the kernel and the function $f$ itself influence the performance.[m
[31m-[m
 \subsection{Error plots}[m
 In this subsection we investigate various errors[m
 $|\hat{f}_{GNW}-f|$ and $|\hat{f}_{GNW}-T_k(f)|$ by simulations.[m
[36m@@ -727,9 +1228,6 @@[m [mOn Figure \ref{fig:error_decay} we plot these errors against the logarithm of th[m
 and bottom left [m
 images [m
 on Figure \ref{fig:error_decay} show empirically that with fixed bandwith  of the kernel the estimator will converge towards $\frac{T_k(f)(x)}{c(x)}$, which in general is at a fixed distance away from $f(x)$ (i.e. in $L^{\infty}$ norm). On the other hand the other images illustrate that in average, these errors decrease as sample size increases. The true errors are lower bounded by the bias term, while the biased errors go to zero.[m
[31m-[m
[31m-[m
[31m-[m
 \begin{figure}[m
     \centering[m
     \includegraphics[width=0.95\textwidth]{gaussian_f_3_20_sim_better_bias_estimate.png}[m
[36m@@ -739,45 +1237,23 @@[m [mon Figure \ref{fig:error_decay} show empirically that with fixed bandwith  of th[m
     \label{fig:error_decay}[m
 \end{figure}[m
 [m
[32m+[m
[32m+[m[32m\subsection{ Estimating functions }[m
[32m+[m
 \begin{figure}[m
     \centering[m
[31m-    \includegraphics[width=0.65\textwidth]{gf3_noise_samplesize_coolwarm.png}[m
[31m-    \caption{True maximum error plotted against sample size and noise level. The parameters are as in Figure \ref{fig:error_decay}.}[m
[31m-    \label{fig:3d_plot}[m
[32m+[m[32m    \includegraphics[width=0.5\textwidth]{SSLAN_f_3_gaussian.png}[m
[32m+[m[32m    \includegraphics[width=0.5\textwidth]{MSMN_f_3_gaussian.png}[m
[32m+[m[32m    \includegraphics[width=0.5\textwidth]{LSLN_f_3_gaussian.png}[m
[32m+[m[32m    \caption{Caption}[m
[32m+[m[32m    \label{fig:my_label}[m
 \end{figure}[m
 [m
 [m
[31m-\subsection{ Estimating functions }[m
[31m-$\mathbb{P}$[m
[31m-$\mathbb{E}$[m
[31m-$\mathbb{a}$[m
[31m-[m
[31m-[m
 \section{Appendix}[m
 [m
 \subsection{related work/future plans}[m
 [m
[31m-[m
[31m-Clustering algorithms on Stochastic block models [m
[31m-[m
[31m-\cite{Oliviera}[m
[31m-[m
[31m-[m
[31m-\cite{Lei_2015}[m
[31m-[m
[31m-[m
[31m-\cite{Levina-Vershynin}[m
[31m-[m
[31m-Latent position model[m
[31m-[m
[31m-\cite{Bickel}[m
[31m-[m
[31m-\cite{Tang}[m
[31m-[m
[31m-\cite{Arias-Castro}[m
[31m-[m
[31m-\cite{Chatterjee}[m
[31m-[m
 \subsection{Probabilistic proof of the Bernoulli inequality}[m
 [m
 The Bernoulli inequality\footnote{In fact the Bernoulli inequality states that for all $y>-1$ and $n\in\mathbb{N}$, $(1+y)^n\geq 1+ny$ but we are only interested in the case $-1<y<0$} states that for all $0<p<1$ and $n\in\mathbb{N}$, [m
[36m@@ -844,346 +1320,8 @@[m [mConsider the sequence $b^{'}_j=\frac{b_j}{j{n\choose j}p^j}$. It is easy to veri[m
 \begin{equation*}[m
     b^{'}_{j+1}=\frac{1}{1}[m
 \end{equation*}[m
[31m-[m
 \end{proof}[m
 [m
[31m-\subsection{Alternative approach to concentration properties}[m
[31m-\subsubsection{Motivation and main ideas}[m
[31m-[m
[31m-\paragraph{Motivation}[m
[31m-Given $x\in\mathbb{R}^d$, as soon as $c(x)>0$, the strong law of large numbers states that [m
[31m-[m
[31m-\begin{equation}[m
[31m-\label{bias_conv}[m
[31m-    \hat{f}_{GNW}(x)\rightarrow b(f,x) \textit{ almost surely}[m
[31m-\end{equation}[m
[31m-[m
[31m-Although  statement (\ref{bias_conv}) is good as a heuristic, it is asymptotic in nature and hence of limited importance for theoretical guarantees such as sample complexities. We use concentration inequalities to specify a rate at which this convergence occurs. This section contains two results, one about concentration properties of $\hat{f}_{GNW}(x)$ with $x\in\mathbb{R}^d$ fixed and such that $c(x)>0$ and one about concentration properties of $\hat{f}_{GNW}(X)$ where $X,X_1,...,X_n$ are i.i.d. random variables with density $p$. We make the following assumptions throughout this section:[m
[31m-[m
[31m-\begin{itemize}[m
[31m-    \item $f\colon\mathbb{R}^d\to\mathbb{R}$ is bounded and measurable with $||f||_{\infty}\leq B$[m
[31m-    \item $x\in\mathbb{R}^d$ is a point with $c(x)=\int k(x,z)p(z)dz>0$[m
[31m-    \item $X,X_1,...,X_n$ are i.i.d. random variables with density $p$[m
[31m-\end{itemize}[m
[31m-[m
[31m-\paragraph{Concentration for a deterministic point}[m
[31m-The Graphical Nadaraya Watson estimator $\hat{f}_{GNW}(x)$ concentrates towards $b(f,x)$[m
[31m-for all bounded functions $f\colon \mathbb{R}^d\to \mathbb{R}$. The concentration is exponential in the number of samples $n$ and  depends on the parameter $c(x)=T_k(1)(x)=\int k(x,z)p(z)dz$. The precise statement of this result is Theorem \ref{thm_1}. The main idea in the proof is to show separately concentration for the statistics $\frac{1}{n}\sum_{i=1}^nY_ia(x,X_i)$ and $\frac{1}{n}\sum_{i=1}^n a(x,X_i)$ towards $T_k(f)(x)$ and $c(x)=T_k(f)(x)$ respectively.[m
[31m-The first step towards Theorem \ref{thm_1} is to show that $\frac{1}{n}\sum_{i=1}^n f(X_i)a(x,X_i)$ concentrates towards $\int f(z)k(x,z)p(z)dz$ using McDiarmid's concentration inequality. This is done in Lemma \ref{basic_lemma_1}. Next, we  need show that the noise term $\frac{1}{n}\sum_{i=1}^n\epsilon_ia(x,X_i)$ concentrates towards $0$. This is done in Lemma \ref{basic_lemma_2} using Subgaussian concentration inequalities. Finally, using Lemma \ref{basic_lemma_1} and Lemma \ref{basic_lemma_2} we prove Theorem \ref{thm_1}.[m
[31m-[m
[31m-\paragraph{Concentration result for a random point} The situation is more involved in the case where $X$ is a random point. The first complications arise out of the fact that the variables $a(X,X_i)$ and $a(X,X_j)$ are no longer independent for $i\neq j$. The concentration  result in this case can be stated informally as[m
[31m-$\hat{f}_{GNW}(X)$ will concentrate towards $b(f,X)$ with a rate that is exponential in $n$ provided that the probability that $c(X)$ takes small values is small. The precise statement of this result is Theorem \ref{thm_2}. The first step towards the proof of Theorem \ref{thm_2} is to derive concentration statements about $\frac{1}{n}\sum_{i=1}^nY_ia(X,X_i)$ and $\frac{1}{n}\sum_{i=1}^n a(X,X_i)$ towards $T_k(f)(X)$ and $c(X)$ respectively. This is done in Corollary \ref{basic_cor_1} and Corollary \ref{basic_cor_2}, which can be considered as the random point analogues of Lemma \ref{basic_lemma_1} and Lemma \ref{basic_lemma_2} respectively. Informally speaking, one can think of Corollary \ref{basic_lemma_1} as integrating the inequality in Lemma \ref{basic_lemma_1} with respect to the density $p$.[m
[31m-[m
[31m-\subsection{Concentration for a deterministic point}[m
[31m-\label{conc_det_pt}[m
[31m-[m
[31m-\begin{lemma}[m
[31m-\label{basic_lemma_1}[m
[31m-Suppose that $f$ is bounded, measurable function with  $||f||_{\infty}\leq B$. Then [m
[31m-\begin{equation*}[m
[31m-P(|\frac{1}{n}\sum_{i=1}^n f(X_i)a(x,X_i)-\int f(z)k(x,z)p(z)dz|\geq t)\leq 2\exp(-\frac{2t^2n}{5B^2})[m
[31m-\end{equation*}[m
[31m-\end{lemma}[m
[31m-\begin{proof} For $i=1,...,n$ we can write $a(x,X_i)=I(U_i\leq k(x,X_i))$ where $U_i$ are i.i.d. uniform  variables on $[0,1]$ independent from the $X_i's$ and $\epsilon_i's$. Define[m
[31m-\begin{equation*}[m
[31m-    F(x_1,...,x_n,u_1,...,u_n)=\frac{1}{n}\sum_{i=1}^n [f(x_i)I(u_i\leq k(x,x_i))-\int f(z)k(x,z)p(z)dz][m
[31m-\end{equation*}[m
[31m-Note that $EF(X_1,...,X_n,U_1,...,U_n)=0$.[m
[31m-We will verify that $F$ satisfies the hypothesis of \\[m
[31m-McDiarmid's bounded difference inequality (\cite{vershynin} Thm 2.9.1). Changing one of the $x_i's$ gives:[m
[31m-[m
[31m-\begin{equation*}[m
[31m-\begin{split}[m
[31m-    &|F(x_1,...,x_i,...,x_n,u_1,...,u_n)-F(x_1,...,x^{'}_i,...,x_n,u_1,...,u_n)|=\\[m
[31m-    &\frac{1}{n}|I(u_i\leq k(x,x_i))f(x_i)-I(u_i\leq k(x,x^{'}_i))f(x^{'}_i)|\leq \frac{2B}{n}[m
[31m-\end{split}[m
[31m-\end{equation*}[m
[31m-[m
[31m-Changing one of the $u_i's$ gives:[m
[31m-[m
[31m-\begin{equation*}[m
[31m-\begin{split}[m
[31m-        &|F(x_1,...,x_n,u_1,...u_i,...,u_n)-F(x_1,...,x_n,u_1,...u^{'}_i,...,u_n)|=\\[m
[31m-    &\frac{1}{n}|[I(u_i\leq k(x,x_i))-I(u^{'}_i\leq k(x,x_i))]f(x_i)|\leq \frac{B}{n}[m
[31m-\end{split}[m
[31m-\end{equation*}[m
[31m-Hence $F$ has the $(c_1,,,c_n,c_{n+1},...,c_{2n})$ bounded difference property with $c_1=c_2=...=c_n=\frac{2B}{n}$ and $c_{n+1}=...=c_{2n}=\frac{B}{n}$, giving $\sum_{i=1}^{2n}c_i^2=\frac{5B^2}{n}$. The result follows immediately from McDiarmid's inequality.[m
[31m-\end{proof}[m
[31m-In the following corollary we prove a concentration result for a variable $X$ which is independent and identically distributed with $X_1,...,X_n$.[m
[31m-[m
[31m-\begin{lemma}[m
[31m-\label{basic_lemma_2}[m
[31m-Suppose that $w_1,...,w_n$ and $\epsilon_1,...,\epsilon_n$ are [m
[31m-independent, $|w_i|\leq 1$ and $\epsilon_i$ are centered Gaussian variables with variance $\sigma^2$. Then[m
[31m-[m
[31m-\begin{equation*}[m
[31m-    P(|\frac{1}{n}\sum_{i=1}^n w_i\epsilon_i|\geq t)\leq 2\exp(-\frac{3ct^2n}{8\sigma^2})[m
[31m-\end{equation*}[m
[31m-[m
[31m-where $c>0$ is an absolute constant.[m
[31m-\end{lemma}[m
[31m-\begin{proof}[m
[31m-Consider the sub-gaussian norm of $w_1\epsilon_1$ defined as [m
[31m-[m
[31m-\begin{equation*}[m
[31m-    ||w_1\epsilon_1||_{\psi_2}=\inf\{t>0: E\exp(w_1\epsilon_1)^2/t^2)\leq 2\}[m
[31m-\end{equation*}[m
[31m-We have[m
[31m-\begin{equation*}[m
[31m-    E\exp( (w_1\epsilon_1)^2/t^2)\leq E\exp(\epsilon_1^2/t^2)=\frac{1}{\sqrt{1-\frac{2\sigma^2}{t^2}}}[m
[31m-\end{equation*}[m
[31m-as soon as $t$ is chosen such that $1-\frac{2\sigma^2}{t^2}>0$. Choosing $t= \sqrt{\frac{8\sigma^2}{3}}$ we get [m
[31m-[m
[31m-\begin{equation*}[m
[31m-    E\exp((w_1\epsilon_1)^2/t^2)\leq 2[m
[31m-\end{equation*}[m
[31m-[m
[31m-In particular this shows that [m
[31m-[m
[31m-\begin{equation*}[m
[31m-    ||w_1\epsilon_1||_{\psi_2}^2\leq \frac{8\sigma^2}{3}[m
[31m-\end{equation*}[m
[31m-[m
[31m-Using the General Hoeffding's inequality (\cite{vershynin} Thm 2.6.3), we have[m
[31m-[m
[31m-\begin{equation*}[m
[31m-    P(|\frac{1}{n}\sum_{i=1}^n w_i\epsilon_i|\geq t)\leq 2\exp(-\frac{3ct^2n}{8\sigma^2})[m
[31m-\end{equation*}[m
[31m-[m
[31m-with $c>0$ an absolute constant. [m
[31m-\end{proof}[m
[31m- [m
[31m- [m
[31m-\begin{theorem}[m
[31m-\label{thm_1}[m
[31m-Suppose that $||f||_{\infty}\leq B$ and $c(x)=Ek(x,X_1)=\int k(x,z)p(z)dz>0$. Then[m
[31m- for $0<\delta<3B$ and [m
[31m-$H(B,\sigma^2)=\min\{\frac{1}{90B^2},\frac{C}{\sigma^2}\}$ we have[m
[31m-[m
[31m-\begin{equation*}[m
[31m-    P(|\hat{f}_{GNW}(x)-\frac{\int f(z)k(x,z)p(z)dz}{\int k(x,z)p(z)dz}|\geq\delta)\leq 6\exp(-H(B,\sigma^2)c(x)^2\delta^2n)[m
[31m-\end{equation*}[m
[31m-\end{theorem}[m
[31m-\begin{proof} [m
[31m-Let $\delta>0$ and denote[m
[31m-[m
[31m-\begin{equation*}[m
[31m-\begin{split}[m
[31m-&A_{\delta}=\{|\frac{1}{n}\sum_{i=1}^n f(x_i)a(x,X_i)-\int f(z)k(x,z)p(z)dz|\geq \delta\}\\[m
[31m-&B_{\delta}=\{|\frac{1}{n}\sum_{i=1}^n a(x,X_i)-c(x)|\geq \delta\}\\[m
[31m-&C_{\delta}=\{|\frac{1}{n}\sum_{i=1}^n\epsilon_ia(x,X_i)|\geq \delta\}[m
[31m-\end{split}[m
[31m-\end{equation*}[m
[31m-Let $\delta_1, \delta_2,\delta_3>0$, we will  specify them later.[m
[31m-Choosing $\delta_2\leq\frac{1}{2}c(x)$, on $B_{\delta_2}^c$ we have[m
[31m-\\[m
[31m-$\frac{1}{n}\sum_{i=1}^na(x,X_i)\geq \frac{1}{2}c(x)$ and in particular $\sum_{i=1}^n a(x,X_i)>0$. Hence on $B_{\delta_2}^c$, we have [m
[31m-[m
[31m-\begin{equation}[m
[31m-\label{eqn:Equation_1}[m
[31m-\begin{split}[m
[31m-    \hat{f}_{GNW}(x)-\frac{\int f(z)k(x,z)p(z)dz}{c(x)}&=\frac{\frac{1}{n}\sum_{i=1}^nY_ia(x,X_i)}{\frac{1}{n}\sum_{i=1}^n a(x,X_i)}-\frac{\int f(z)k(x,z)p(z)dz}{c(x)}\\[m
[31m-    &=\frac{\frac{1}{n}\sum_{i=1}^n[f(X_i)-\frac{T_k(f)(x)}{c(x)}]a(x,X_i)}{\frac{1}{n}\sum_{i=1}^n a(x,X_i)}+\frac{\frac{1}{n}\sum_{i=1}\epsilon_ia(x,X_i)}{\frac{1}{n}\sum_{i=1}^n a(x,X_i)}[m
[31m-\end{split}[m
[31m-\end{equation}[m
[31m-In addition, on $(A_{\delta_1}\cup B_{\delta_2}\cup C_{\delta_3})^c$, we have[m
[31m-\begin{equation*}[m
[31m-\begin{split}[m
[31m-    |\hat{f}_{GNW}(x)-\frac{\int f(z)k(x,z)p(z)dz}{c(x)}|&\leq |\frac{\frac{1}{n}\sum_{i=1}^n[f(X_i)-\frac{T_k(f)(x)}{c(x)}]]a(x,X_i)}{\frac{1}{n}\sum_{i=1}^n a(x,X_i)}|+|\frac{\frac{1}{n}\sum_{i=1}\epsilon_ia(x,X_i)}{\frac{1}{n}\sum_{i=1}^n a(x,X_i)}|\\[m
[31m-    &= |\frac{\frac{1}{n}\sum_{i=1}^n [f(X_i)c(x)-T_k(f)]a(x,X_i)}{\frac{1}{n}c(x)\sum_{i=1}^na(x,X_i)}|+|\frac{\frac{1}{n}\sum_{i=1}\epsilon_ia(x,X_i)}{\frac{1}{n}\sum_{i=1}^n a(x,X_i)}|\\[m
[31m-    & = |\frac{c(x)[\frac{1}{n}\sum_{i=1}^nf(X_i)a(x,X_i)-T_k(f)(x)]+T_k(f)(x)[c(x)-\frac{1}{n}\sum_{i=1}^n a(x,X_i)][m
[31m-    }{\frac{1}{n}c(x)\sum_{i=1}^na(x,X_i)}\\[m
[31m-    &+|\frac{\frac{1}{n}\sum_{i=1}\epsilon_ia(x,X_i)}{\frac{1}{n}\sum_{i=1}^n a(x,X_i)}|\\[m
[31m-    & \leq  \frac{c(x)(\delta_1+\delta_3)+T_k(f)(x)\delta_2}{\frac{1}{n}c(x)\sum_{i=1}^n a(x,X_i)}\\[m
[31m-    & \leq \frac{c(x)(\delta_1+B\delta_2+\delta_3)}{\frac{1}{n}c(x)\sum_{i=1}^n a(x,X_i)}\\[m
[31m-    & \leq \frac{2(\delta_1+\delta_2B+\delta_3)}{c(x)}[m
[31m-\end{split}[m
[31m-\end{equation*}[m
[31m-Finally, setting[m
[31m-[m
[31m-\begin{equation*}[m
[31m-\delta_1=\delta_3=\frac{\delta c(x)}{6}, \textit{ } \delta_2=\frac{\delta c(x)}{6B}[m
[31m-\end{equation*}[m
[31m-[m
[31m-we get [m
[31m-[m
[31m-\begin{equation*}[m
[31m-     |\hat{f}_{GNW}(x)-\frac{\int f(z)k(x,z)p(z)dz}{\int k(x,z)p(z)dz}|\leq \delta[m
[31m-\end{equation*}[m
[31m-[m
[31m-on  $(A_{\delta_1}\cup B_{\delta_2}\cup C_{\delta_3})^c$.[m
[31m-\\[m
[31m-By Lemma \ref{basic_lemma_1}, we have $P(A_{\delta_1})\leq 2\exp(-\frac{2\delta_1^2n}{5B^2})$ and[m
[31m-$P(B_{\delta_2})\leq 2\exp(-\frac{2\delta_2^2n}{5})$.[m
[31m-\\[m
[31m-By Lemma \ref{basic_lemma_2} we have[m
[31m-$P(C_{\delta_3})\leq 2\exp(-\frac{C\delta_3^2n}{\sigma^2})$ where $C>0$ is a constant.[m
[31m-[m
[31m-The result follows from a union bound [m
[31m-\begin{equation*}[m
[31m-\begin{split}[m
[31m-    P(A_{\delta1}\cup B_{\delta_2}\cup C_{\delta_3})&\leq P(A_{\delta_1})+P(B_{\delta_2})+P(C_{\delta_3})\\[m
[31m-    &\leq 6\exp(-H(B,\sigma^2)c(x)^2\delta^2n)[m
[31m-\end{split}[m
[31m-\end{equation*}[m
[31m-\end{proof}[m
[31m-[m
[31m-\subsection{Concentration for a random point}[m
[31m-\label{conc_random_pt}[m
[31m-[m
[31m-\begin{corollary}[m
[31m-\label{basic_cor_1}[m
[31m-Suppose that $f$ is bounded, measurable function with $||f||_{\infty}\leq B$ and that $X,X_1,...,X_n$ are i.i.d. random variables with density $p$.[m
[31m-Then[m
[31m-\begin{equation*}[m
[31m-    P(|\frac{1}{n}\sum_{i=1}^nf(X_i)a(X_i,X)-\int f(z)k(X,z)p(z)dz|\geq t)\leq 2\exp(-\frac{2t^2n}{5B^2})[m
[31m-\end{equation*}[m
[31m-\end{corollary}[m
[31m-\begin{proof}[m
[31m-Let $U_1,...,U_n$ be i.i.d. uniform on $[0,1]$ such that $a(X,X_i)=I(U_i\leq k(X,X_i))$. Consider the indicator function $\phi:\mathbb{R}^{2n+1}\rightarrow\mathbb{R}$ given by [m
[31m-[m
[31m-\begin{equation*}[m
[31m-    \phi(x,x_1,...,x_n,u_1,...,u_n)=I(|\frac{1}{n}\sum_{i=1}^n f(x_i)I(u_i\leq k(x,x_i))-\int f(z)k(x,z)p(z)dz|\geq t)[m
[31m-\end{equation*}[m
[31m-According to \ref{basic_lemma_1}, we have[m
[31m-[m
[31m-\begin{equation*}[m
[31m-\begin{split}[m
[31m-    E\phi(x,X_1,...,X_n,U_1,...,U_n)&= \int \phi(x,x_1,...,x_n,u_1,...,u_n)\prod_{i=1}^n p(x_i)\prod_{i=1}^n dx_i\prod_{i=1}^n du_i\\[m
[31m-    &=P(|\frac{1}{n}\sum_{i=1}^n f(X_i)a(x,X_i)-\int f(z)k(x,z)p(z)dz|\geq t)\\[m
[31m-    &\leq 2\exp(-\frac{2t^2n}{5B^2})[m
[31m-\end{split}[m
[31m-\end{equation*}[m
[31m-Finally, using Fubini's theorem, we have[m
[31m-[m
[31m-\begin{equation*}[m
[31m-    \begin{split}[m
[31m-        P(|\frac{1}{n}\sum_{i=1}^nf(X_i)a(X_i,X)-\int f(z)k(X,z)p(z)dz|\geq t)&=E\phi(X,X_1,...,X_n,U_1,...,U_n)\\[m
[31m-        &=\int [\phi(x,x_1,..,x_n.u_1,...,u_n)\prod_{i=1}^n p(x_i)\prod_{i=1}^n dx_i\prod_{i=1}^n du_i]p(x)dx\\[m
[31m-        &=\int E\phi(x,X_1,...,X_n,U_1,...,U_n)p(x)dx\\[m
[31m-        &\leq 2\exp(-\frac{2t^2n}{5B^2})\int  p(x)dx\\[m
[31m-        &=2\exp(-\frac{2t^2n}{5B^2})[m
[31m-    \end{split}[m
[31m-\end{equation*}[m
[31m-\end{proof}[m
[31m-[m
[31m-\begin{corollary}[m
[31m-\label{basic_cor_2}[m
[31m-Suppose that and $\epsilon_1,...,\epsilon_n$ are i.i.d. centered Gaussian variables with variance $\sigma^2$, $X,X_1,...,X_n$ are i.i.d. with density $p$. Then[m
[31m- [m
[31m- \begin{equation*}[m
[31m-    P(|\frac{1}{n}\sum_{i=1}^n \epsilon_ia(X,X_i)|\geq t)\leq 2\exp(-\frac{3ct^2n}{8\sigma^2})[m
[31m- \end{equation*}[m
[31m-\end{corollary}[m
[31m- \begin{proof}[m
[31m- The result follows by Lemma  \ref{basic_lemma_2}  using the same method that was used to derive Corollary \ref{basic_cor_1} from Lemma \ref{basic_lemma_1}. [m
[31m- \end{proof}[m
[31m-[m
[31m-\begin{theorem}[m
[31m-\label{thm_2}[m
[31m-Suppose that $||f||_{\infty}\leq B$ and [m
[31m-$X,X_1,...,X_n$ are i.i.d. random variables [m
[31m-with density $p$.[m
[31m-Set $H(B,\sigma^2)=\min(\frac{c_1}{\sigma^2},\frac{1}{90B^2})$. Then [m
[31m-for all $r>0$ and $0<\delta<3B$ we have [m
[31m-[m
[31m-\begin{equation*}[m
[31m-    P(c(X)>0,|\hat{f}_{GNW}(X)-\frac{T_k(f)(X)}{c(X)}|\geq\delta)\leq 6\exp(-H(B,\sigma^2)r^2\delta^2n)+P(c(X)<r)[m
[31m-\end{equation*}[m
[31m-[m
[31m-\end{theorem}[m
[31m-[m
[31m-\begin{proof}[m
[31m-[m
[31m-For $\delta,r>0$ and $f:\mathbb{R}^d\rightarrow \mathbb{R}$ bounded, let [m
[31m-\begin{equation*}[m
[31m-    \begin{split}[m
[31m-        &C_r=\{\int k(X,z)p(z)dz\geq r\}=\{c(X)\geq r\}\\[m
[31m-        &A_{\delta}(f)=\{|\frac{1}{n}\sum_{i=1}^n f(X_i)a(X,X_i)-\int f(z)k(X,z)p(z)dz|\geq \delta\}\\[m
[31m-        &N_{\delta}=\{|\frac{1}{n}\sum_{i=1}^n \epsilon_ia(X,X_i)|\geq \delta\}[m
[31m-    \end{split}[m
[31m-\end{equation*}[m
[31m-[m
[31m-Let $\delta_1,\delta_2,\delta_3>0$ to be specified later.[m
[31m-On $C_r\cap A_{\delta_1}(f)^c\cap A_{\delta_2[m
[31m-}(1)^c\cap N_{\delta}^c$ we have [m
[31m-[m
[31m-\begin{equation*}[m
[31m-    \frac{1}{n}\sum_{i=1}^n a(X,X_i)>c(X)-\delta_2\geq r-\delta_2\geq \frac{r}{2}[m
[31m-\end{equation*}[m
[31m-as soon as $\delta_2<\frac{r}{2}$. Furthermore the same calculation as in Theorem \ref{thm_1}[m
[31m-gives[m
[31m-[m
[31m-\begin{equation*}[m
[31m-\begin{split}[m
[31m-     |\hat{f}_{GNW}(X)-\frac{\int f(z)k(X,z)p(z)dz}{c(X)}|&\leq \frac{\delta_1+\delta_3+\delta_2B}{\frac{1}{n}\sum_{i=1}^n a(X,X_i)}\\[m
[31m-     &\leq 2\frac{\delta_1+\delta_3+\delta_2B}{r}[m
[31m-\end{split}[m
[31m-\end{equation*}[m
[31m-Choosing $\delta_1=\delta_3=[m
[31m-\frac{r\delta}{6}$ and $\delta_2=\min(\frac{r}{2},\frac{r\delta}{6B})$, on $C_r\cap A_{\delta_1}(f)^c\cap A_{\delta_2[m
[31m-}(1)^c\cap N_{\delta}^c$ we have $c(X)>0$ and[m
[31m-[m
[31m-\begin{equation*}[m
[31m-    |\hat{f}_{GNW}(X)-\frac{\int f(z)k(X,z)p(z)dz}{c(X)}|\leq \delta[m
[31m-\end{equation*}[m
[31m-To conclude, when $\delta<3B$, using Corollary \ref{basic_cor_1} and Corollary \ref{basic_cor_2} we have[m
[31m-[m
[31m-[m
[31m-\begin{equation*}[m
[31m-\begin{split}[m
[31m-    &P(c(X)>0,|\hat{f}_{GNW}(X)-\frac{T_k(f)(X)}{c(X)}|\geq\delta)\leq P(C_r^c\cup A_{\delta_1}(f)\cup A_{\delta_2[m
[31m-}(1)\cup N_{\delta})\\&\leq P(c(X)<r)+2\exp(-\frac{r^2\delta^2n}{90B^2})+2\exp(-\frac{r^2\delta^2n}{90B^2})+2\exp(-\frac{c_1r^2\delta^2n}{\sigma^2})\\[m
[31m-&\leq P(c(X)<r)+6\exp(-H(B,\sigma^2)r^2\delta^2n)[m
[31m-\end{split}[m
[31m-\end{equation*}[m
[31m-[m
[31m-\end{proof}[m
[31m-\subsection{Remarks}[m
[31m-[m
[31m-\begin{remark}[m
[31m-The quantity $H(B,\sigma^2)$ is inversly proportional to the boundedness and noise parameters $B$ and $\sigma^2$ respectively. In particular if either of the two parameters $B$ and $\sigma^2$ increases while the other is fixed, the quantity  $H(B,\sigma^2)=\min(\frac{C}{\sigma^2},\frac{1}{90B^2})$ decreases and hence the concentration rate in Theorem \ref{thm_1} and Theorem \ref{thm_2} decreases.[m
[31m-\end{remark}[m
[31m-[m
[31m-[m
[31m-\begin{remark}[m
[31m-The proof of Lemma \ref{basic_lemma_2} relies on sub-gaussian inequalities. These inequalities hold for a wider class of probability distributions, namely for subgaussian variables. Thus similar results hold if one assumes that the variables $\epsilon_i$ are i.i.d subgaussian.[m
[31m-\end{remark}[m
[31m-[m
[31m-\begin{remark}[m
[31m-As long as $E|f(X_1)k(x,X_1)|=\int |f(z)|k(x,z)p(z)dz<\infty $ and $c(x)=\int k(x,z)p(z)dz>0$, the strong law of large numbers states that [m
[31m-[m
[31m-\begin{equation*}[m
[31m-    \hat{f}_{GNW}(x)\rightarrow \frac{\int f(z)k(x,z)p(z)dz}{\int k(x,z)p(z)dz}[m
[31m-\end{equation*}[m
[31m-This is the case if $E|f(X_1)|=\int f(z)p(z)dz<\infty$.[m
[31m-However, it is not clear how to obtain concentration results for such a weak assumption. Under weaker assumption such as $f(X_1)\in L^2$ one can use Chebyshev or Markov inequalities to find a concentration rate.  One way to slightly generalize the function class (while preserving the strong concentration rate) is to consider functions $f$ for which $f(X_1)$ is sub-gaussian i.e. there exists $t>0$ such that [m
[31m-[m
[31m-\begin{equation*}[m
[31m-    E\exp(\frac{f^2(X_1)}{t^2})=\int \exp(\frac{f^2(z)}{t^2})p(z)dz<\infty  [m
[31m-\end{equation*}[m
[31m-[m
[31m-With such an assumption on $f$ it is possible to replace McDiarmid's bounded difference inequality with Hoeffding's inequality to obtain similar concentration result, where the constant $B$ is replaced by an upper bound of the $\psi_2$ subgaussian norm $\psi_2(X_1)$[m
[31m-\end{remark}[m
[31m-[m
[31m-\begin{remark} A slight modification of the presented proofs shows that the classical Nadaraya Watson estimator $\hat{f}_{NW}$ given by \ref{NW_def} satisfies [m
[31m-\begin{equation*}[m
[31m-    P(|\hat{f}_{GNW}(x)-\hat{f}_{NW}(x)|\geq \delta)\leq c_1\exp(-H_1(B,\sigma^2)c(x)^2\delta^2n)[m
[31m-\end{equation*}[m
[31m-for some absolute constants $c_1>0$ and $H_1(B,\sigma^2)>0$.[m
[31m-Indeed, if we take[m
[31m-\begin{equation*}[m
[31m-    F_1(x_1,...x_n)=\frac{1}{n}\sum_{i=1}^n [f(x_i)k(x,x_i)-\int f(z)k(x,z)p(z)dz][m
[31m-\end{equation*}[m
[31m-then $EF_1(X_1,...,X_n)=0$ and similar ideas as in Lemma \ref{basic_lemma_1} apply. We omit the details. Note that the ambient space in which the data  $X_1,...,X_n$ is embedded does not a play a role in this section as long as the variables are independent and $||f||_{\infty}\leq B$. In particular the dimensionality of the data plays no role in the approximation of $\hat{f}_{NW}$ by $\hat{f}_{GNW}$. However, we still have to take into account that our ultimate goal is to estimate $f$, and not $\hat{f}_{NW}$, and the dimensionality of the data will play an important role here as we show in the next section.[m
[31m-\end{remark}[m
[31m-[m
[31m-[m
[31m-[m
[31m-[m
[31m-\begin{remark}[m
[31m-Assuming that $\inf_{x\in\supp{p}} c(x)\geq r>0$ gives $P(\int k(X,z)p(z)dz<r)=0$ so that by Theorem \ref{thm_2}, $\hat{f}_{GNW}(X)$ concentrates around $\frac{\int f(z)k(X,z)p(z)dz}{\int k(X,z)p(z)dz}$ with exponentially small probability in the sample size $n$. An application of Borel-Cantelli's lemma in such a case gives that $\hat{f}_{GNW}(X)\rightarrow \frac{T_k(f)(X)}{T_k(1)(X)}$ almost surely. This is the case For example $p(z)$ is compactly supported density (i.e. the data $X_1,...,X_n$ are drawn i.i.d. from some compact set) and $c(x)>0$ for all $x$ in the support of $p$. In general, there is a penalty term $P(\int k(X,z)p(z)dz<r)$ which is highly dependent on the kernel $k$. However it is still true that $\hat{f}_{GNW}(X)$ converges in probability towards $\frac{\int f(z)k(X,z)p(z)dz}{c(X)}$.[m
[31m-\end{remark}[m
 [m
 [m
 \subsection{A Generalization: Higher order GNW estimators}[m
[36m@@ -1198,164 +1336,38 @@[m [m$v$. In order to account for the potential influence of vertices which are not d[m
 bounded and [m
 independent, their contribution will vanish for large n so to simplify the exposition we assume that $a(X_i,X_i)=0$.[m
 }[m
[32m+[m[32m\paragraph{Remark 8 (Higher order GNW Estimators)}[m
[32m+[m[32mGiven $1\leq m\leq n$, we introduce the weights[m
 [m
[31m-[m
[31m-\begin{equation*}[m
[31m-    w_{2}(X_i,X)=\sum_{j=1,j\neq i}^na(X_i,X_j)a(X_j,X)[m
[31m-\end{equation*}[m
[31m-[m
[31m-We introduce the \textbf{Second order GNW estimator}:[m
[31m-[m
[31m-\begin{equation*}[m
[31m-    \hat{f}_{GNW,2}(x)=\frac{\sum_{i=1}^n Y_i[m
[31m-    w_2(X_i,x)}{\sum_{i=1}^n w_2(X_i,x)}[m
[31m-\end{equation*} [m
[31m-\paragraph{Lemma 6} Suppose that $||f(X_1)||_{\infty}\leq B$, and $X,X_1,...,X_n$ are i.i.d. with density $p$. Then[m
[31m-[m
[31m-\begin{equation*}[m
[31m-     P(|\frac{1}{n(n-1)}\sum_{i=1}^n f(X_i)w_2(X_i,X)-T_k^2(f)(X)|\geq 2\delta)\leq (2n+2)\exp(\frac{-2\delta^2(n-1)}{5B})[m
[31m-\end{equation*}[m
[31m-[m
[31m-\begin{proof}[m
[31m-Set [m
[31m-[m
[31m-\begin{equation*}[m
[31m-    S_j=\frac{1}{n-1}\sum_{i\neq j}f(X_i)a(X_i,X_j)[m
[31m-\end{equation*}[m
[31m-[m
[31m-We compute:[m
[31m-[m
[31m-\begin{equation}[m
[31m-\label{eqn:Equation_4}[m
[31m-\begin{split}[m
[31m-    \frac{1}{n(n-1)}\sum_{i=1}^n f(X_i)w_2(X_i,X)&[m
[31m-    =\frac{1}{n(n-1)}\sum_{j=1}^n[\sum_{i\neq j} f(X_i)a(X_i,X_j)]a(X_j,X)\\[m
[31m-    &=\frac{1}{n}\sum_{j=1}^n [\frac{1}{n-1}\sum_{i\neq j} f(X_i)a(X_i,X_j)-\int f(z)k(X_j,z)p(z)dz]a(X_j,X)\\[m
[31m-    & +\frac{1}{n}\sum_{j=1}^n[\int f(z)k(X_j,z)p(z)dz]a(X_j,X)\\[m
[31m-    &=\frac{1}{n}\sum_{j=1}^n [S_j-T_k(f)(X_j)]a(X_j,X)+\frac{1}{n}\sum_{j=1}^n T_k(X_j)a(X_j,X)[m
[31m-\end{split}[m
[31m-\end{equation}[m
[31m-[m
[31m-Given $1\leq j\leq n$, according to Corolary 1 applied to the $n-1$ variables $X_1,...,X_{j-1},X_{j+1},...,X_n$, we have[m
 \begin{equation*}[m
[31m-P(|S_j-T_k(f)(X_j)|\geq \delta)\leq 2\exp(-\frac{2\delta^2(n-1)}{5B})[m
[32m+[m[32m    w_m(X_i,X)=\sum_{J_{i}} \prod_{j=0}^{m-1} a(X_{i_j},X_{i_{j+1}})[m
 \end{equation*}[m
[31m-Hence, by a union bound we have [m
[32m+[m[32mHere, $J_{i}=(i,i_1,...,i_{m-1})$ is a $m$-tuple of distinct indicies with the convention that[m
[32m+[m[32m$i_0=i$ and $X_{i_m}$ is identified with $X$ and the sum is taken over all such $m$-tuples $J_i$. We introduce the \textbf{GNW estimator of order m}:[m
 \begin{equation*}[m
[31m-\begin{split}[m
[31m-    P(|\frac{1}{n}\sum_{j=1}^n [S_j-T_k(f)(X_j)]a(X_j,X)|\geq\delta)[m
[31m-    )&\leq \sum_{i=1}^n P(|S_j-T_k(f)(X_j)|\geq \delta)\\[m
[31m-    &\leq 2n\exp(-\frac{2\delta^2(n-1)}{5B})[m
[31m-\end{split}[m
[32m+[m[32m    \hat{f}_{GNW,m}(X)=\frac{\sum_{i=1}^n Y_iw_m(X_i,X)}{\sum_{i=1}^n w_m(X_i,X)}[m
 \end{equation*}[m
[31m-Applying Corolary 1 with $f_1(x)=T_k(f)(x)=\int f(z)k(x,z)p(z)dz$ (which is also bounded by $B$), we have[m
[32m+[m[32mThe case $m=2$ which was discussed in the previous paragraph is can be used as an inductive step in proving a concentration inequality for $\hat{f}_{GNW,m}$. It can be shown in a simmilar manner in which Lemma 6 was shown that[m[41m [m
[32m+[m[32mAs $pdx$ is a probability measure, compositions of $T_k$ of any order[m
[32m+[m[32m$m\geq 1$[m
[32m+[m[32mare well defined, and[m[41m [m
 [m
 \begin{equation*}[m
[31m-    P(|\frac{1}{n}\sum_{j=1}^nT_k(f)(X_j)[m
[31m-    a(X_j,X)-T_k^2(f)(X)|\geq \delta)\leq 2\exp(-\frac{2\delta^2n}{5B})[m
[31m-\end{equation*}[m
[31m-Finally, combining the last two displays together with (\ref{eqn:Equation_4}), we have[m
[31m-\begin{equation*}[m
[31m-\begin{split}[m
[31m-    P(|\frac{1}{n(n-1)}\sum_{i=1}^n f(X_i)w_2(X_i,X)-T_k^2(f)(X)|\geq 2\delta)&\leq P(|\frac{1}{n}\sum_{j=1}^n [S_j-T_k(f)(X_j)]a(X_j,X)|\geq\delta)\\& +P(|\frac{1}{n}\sum_{j=1}^nT_k(f)(X_j)[m
[31m-    a(X_j,X)-T_k^2(f)(X)|\geq \delta)\\[m
[31m-    &\leq (2n+2)\exp(-\frac{2\delta^2(n-1)}{5B})[m
[31m-\end{split}[m
[31m-\end{equation*}[m
[31m-\end{proof}[m
[31m-\paragraph{Theorem 4} For any $r>0$, [m
[31m-\begin{equation*}[m
[31m-  P(|\hat{f}_{GNW,2}(X)-\frac{T_k^2(f)(X)}{c_2(X)}|[m
[31m-   \geq \frac{(4r+2)\delta}{r^2})\leq P(c_2(X)<r)+c_1n\exp(-H(B,\sigma^2)\delta^2(n-1))[m
[31m-\end{equation*}[m
[31m-\begin{proof}[m
[31m-Denote [m
[31m-\begin{equation*}[m
[31m-\begin{split}[m
[31m-    &C_r=\{c_2(X)\geq r\}=\{\int\int k(X,w)k(w,z)p(w)p(z)dwdz\geq r\}\\[m
[31m-    &A_{\delta}(f)=\{|\frac{1}{n(n-1)}\sum_{i=1}^n f(x_i)w_2(x,X_i)-T_k^2(f)(X)|\geq \delta\}\\[m
[31m-    &N_{\delta}=\{|\frac{1}{n(n-1)}\sum_{i=1}^n \epsilon_i w_2(X_i,X)|\geq \delta\}[m
[31m-\end{split}[m
[31m-\end{equation*}[m
[31m-As soon as $\delta<\frac{r}{2}$, on $C_r\cap A_{\delta}(1)^c$ we have[m
[31m-\begin{equation*}[m
[31m-    \begin{split}[m
[31m-        \hat{f}_{GNW,2}(X)[m
[31m-        &=\frac{\frac{1}{n(n-1)}\sum_{i=1}^n f(X_i)w_2(X_i,X)-T_k^2(f)(X)}{\frac{1}{n(n-1)}[m
[31m-        \sum_{i=1}^n w_2(X,X_i)}\\[m
[31m-        &+\frac{[m
[31m-        T_k^2(f)(X)}{\frac{1}{n(n-1)}\sum_{i=1}^n w_2(X_i,X)}+\frac{\sum_{i=1}^n \epsilon_iw_2(X_i,X)}{\sum_{i=1}^n w_2(X_i,X)}[m
[31m-    \end{split}[m
[31m-\end{equation*}[m
[31m-and [m
[31m-\begin{equation*}[m
[31m-    \frac{1}{\frac{1}{n(n-1)}w_2(X_i,X)}\leq \frac{2}{r}[m
[32m+[m[32m    T_k^m(f)(x)=\int_{\mathbb{R}^d} T_k^{m-1}(f(z))k(x,z)p(z)dz[m
 \end{equation*}[m
[31m-In particular $\hat{f}_{GNW,2}(X)$ is well defined on $C_r\cap A_{\delta}(1)^c$ [m
[31m-[m
[31m-Using the same technique as in Lemma 6, together with subgaussian concentration inequalities we can show that\footnote{ The technical details can be provided later if necessary}[m
 [m
[31m-\begin{equation*}[m
[31m-    P(|\frac{1}{n(n-1)}\sum_{i=1}^n \epsilon_i w_2(X_i,X)|\geq \delta)\leq c_1n\exp(-C(\sigma^2)\delta^2(n-1)) [m
[31m-\end{equation*}[m
[31m-where $c_1,C(\sigma^2)>0$.[m
[31m-\\[m
[31m-On $C_r\cap A_{\delta}(1)^c\cap A_{\delta}(f)^c$ we have[m
 [m
[31m-\begin{equation*}[m
[31m-    \begin{split}[m
[31m-    |\frac{\frac{1}{n(n-1)}\sum_{i=1}^n f(X_i)w_2(X_i,X)-\int \int f(z)k(w,z)k(w,X)p(z)p(w)dzdw[m
[31m-    }{\frac{1}{n(n-1)}[m
[31m-    \sum_{i=1}^n w_2(X,X_i)}|&\leq \frac{2\delta}{r}[m
[31m-    \end{split}[m
[31m-\end{equation*}[m
[31m-Next, on $C_r\cap A_{\delta}(1)^c$ we have [m
[31m-\begin{equation*}[m
[31m-    |\frac{1}{\frac{1}{n(n-1)}\sum_{i=1}^n w_2(X_i,X)}[m
[31m-    -\frac{1}{\int\int k(X,z)k(z,w)p(z)p(w)dzdw}|\leq \frac{2}{r^2}\delta[m
[31m-\end{equation*}[m
[31m-Finally, [m
[31m-on $C_r\cap A_{\delta}(1)^c\cap A_{\delta}(f)^c\cap N_{\delta}^c$ we have[m
 [m
[31m-\begin{equation*}[m
[31m-    \begin{split}[m
[31m-         |\hat{f}_{GNW,2}(X)-\frac{T_k^2(f)(X)}{c_2(X)}|\leq \frac{4\delta}{r}+\frac{2\delta}{r^2}[m
[31m-    \end{split}[m
[31m-\end{equation*}[m
[31m-We conclude with a union bound[m
[31m-\begin{equation*}[m
[31m-    P(C_r^c\cup A_{\delta}(1)\cup A_{\delta}(f)\cup N_{\delta})\leq P(C_r^c)+c_1n\exp(-H(B,\sigma^2)\delta^2(n-1))[m
[31m-\end{equation*}[m
[31m-\end{proof}[m
[31m-\paragraph{Corollary 4} If $r=\inf_{x\in \supp p} c_2(x)>0$ then[m
 [m
 \begin{equation*}[m
[31m-   P(|\hat{f}_{GNW,2}(X)-\frac{T_k^2(f)(X)}{c_2(X)}|[m
[31m-   \geq \frac{(4r+2)\delta}{r^2})\leq c_1n\exp(-H(B,\sigma^2)\delta^2(n-1))[m
[32m+[m[32mP(|\frac{(n-m)!}{n!}\sum_{i=1}^n f(X_i)w_m(X_i,X)-\frac{(n-(m-1))!}{n!}\sum_{i=1}^n T_k(f)(X_i)w_{m-1}(X_i,X)|\geq \delta)\leq 2n^{m-1}\exp(-\frac{2\delta^2(n-(m))}{5B})[m
 \end{equation*}[m
[31m-\begin{proof}[m
[31m-Follows immediately from Theorem 3.[m
[31m-\end{proof}[m
[32m+[m[32m\paragraph{Remark 9 (Application to the Stochastic Block Model)}[m
[32m+[m[32mMore formally, $SBM(n,p,W)$ where $n$ is a positive integer, $p=(p_1,...,p_K)$ is $K$ dimensional vector with $0\leq p_l\leq 1$ and $\sum_{l=1}^Kp_l=1$ and $W$ is a $K\times K$ symmetric matrix with entries $0\leq w_{i,j}\leq 1$ generates edges on vertices $[n]$ by first randomly assigning a community $C_1,...,C_K$ to each node with $P(i\in C_l)=p_l$ (these assignments are independent over distinct vertices) and then generating edges depending on the community of the endpoints of the edge, that is $P(i\sim j| i\in C_l,j\in C_s)=w_{ls}$. There are several questions in the $SBM$ model such as deciding if an observed graph is indeed sampled by an $SBM$, under which conditions is there a way to fully or partially recover communities based on a single observed graph. The Stochastic Block Model has a long history in the statistics literature (  \cite{Snijders}). For recent developments we refer to (\cite{Abbe}).[m
 [m
[31m-\paragraph{Remarks}[m
 [m
[31m-\paragraph{Remark 8 (Higher order GNW Estimators)}[m
[31m-Given $1\leq m\leq n$, we introduce the weights[m
[31m-[m
[31m-\begin{equation*}[m
[31m-    w_m(X_i,X)=\sum_{J_{i}} \prod_{j=0}^{m-1} a(X_{i_j},X_{i_{j+1}})[m
[31m-\end{equation*}[m
[31m-Here, $J_{i}=(i,i_1,...,i_{m-1})$ is a $m$-tuple of distinct indicies with the convention that[m
[31m-$i_0=i$ and $X_{i_m}$ is identified with $X$ and the sum is taken over all such $m$-tuples $J_i$. We introduce the \textbf{GNW estimator of order m}:[m
[31m-\begin{equation*}[m
[31m-    \hat{f}_{GNW,m}(X)=\frac{\sum_{i=1}^n Y_iw_m(X_i,X)}{\sum_{i=1}^n w_m(X_i,X)}[m
[31m-\end{equation*}[m
[31m-The case $m=2$ which was discussed in the previous paragraph is can be used as an inductive step in proving a concentration inequality for $\hat{f}_{GNW,m}$. It can be shown in a simmilar manner in which Lemma 6 was shown that [m
 [m
[31m-\begin{equation*}[m
[31m-P(|\frac{(n-m)!}{n!}\sum_{i=1}^n f(X_i)w_m(X_i,X)-\frac{(n-(m-1))!}{n!}\sum_{i=1}^n T_k(f)(X_i)w_{m-1}(X_i,X)|\geq \delta)\leq 2n^{m-1}\exp(-\frac{2\delta^2(n-(m))}{5B})[m
[31m-\end{equation*}[m
[31m-\paragraph{Remark 9 (Application to the Stochastic Block Model)} The stochastic block model $SBM(n,W,p)$ (where $n$ is a positive integer, $W$ a $k\times k$ symmetric matrix with entries in $[0,1]$ and[m
[32m+[m[32mThe stochastic block model $SBM(n,W,p)$ (where $n$ is a positive integer, $W$ a $k\times k$ symmetric matrix with entries in $[0,1]$ and[m
 $p=(p_1,...,p_k)$ is such that[m
 $p_1+...+p_k=1$ and $p_i>0$, $i=1,...,n$)[m
 is a random graph model that can be defined as follows:[m
[36m@@ -1381,109 +1393,6 @@[m [mit follows that edge related statistics such as $w_m(i,j)$ can identyify the ind[m
 [m
 [m
 [m
[31m-\section{maybe useful?}[m
[31m-As $pdx$ is a probability measure, compositions of $T_k$ of any order[m
[31m-$m\geq 1$[m
[31m-are well defined, and [m
[31m-[m
[31m-\begin{equation*}[m
[31m-    T_k^m(f)(x)=\int_{\mathbb{R}^d} T_k^{m-1}(f(z))k(x,z)p(z)dz[m
[31m-\end{equation*}[m
[31m-[m
[31m-\paragraph{ remarks? }[m
[31m- If $\int ||y||^2k(y)dy<\infty$,[m
[31m-    $p$ is $\beta$ Holder continuous, with $0<\beta\leq 1$ and $p(x)>0$ then there exists $n(x)\in \mathbb{N}$ such that for all $n\geq n(x)$,[m
[31m-    \begin{equation*}[m
[31m-        |\frac{T_{k_n}(f)(x)}{T_{k_n}(1)(x)}-f(x)|\leq ch_n^{\alpha}[m
[31m-    \end{equation*}[m
[31m-    with $c>0$ an absolute constant depending on $k$ and the Holder constants of $f$ and $p$.[m
[31m-\paragraph{remarks..}[m
[31m-\begin{itemize}[m
[31m-    \item Suppose now that $k$ is compactly supported and let $M>0$ be a constant such that $k(x)=0$ for $|x|>M$. Then, from (\ref{eqn:bias}) we get [m
[31m-\begin{equation*}[m
[31m-    |\frac{T_{k_n}(f)(x)}{T_{k_n}(1)(x)}-f(x)|\leq LM^{\alpha}h_n^{\alpha}[m
[31m-\end{equation*}[m
[31m-This bound is independent of $x\in\supp{p}$ and of $f$ in the Holder class, and this proves the first claim.[m
[31m-    \item Now suppose $p(x)>0$ and $p$ is $\beta$-Holder continuous. Then for any $G(y)$ such that both [m
[31m-    $\int |G(y)|k(y)dy<\infty$ and $\int ||y||^{\beta}|G(y)|k(y)dy<\infty$ we have[m
[31m-    \begin{equation}[m
[31m-    \[m
[31m-    \begin{split}[m
[31m-        |\int G(y)k(y)p(x+h_ny)dy-p(x)\int G(y)k(y)dy|&=|\int G(y)k(y)[p(x+h_ny)-p(x)]dy|\\[m
[31m-        &\leq h_n^{\beta}\int ||y||^{\beta}|G(y)|k(y)dy[m
[31m-    \end{split}[m
[31m-    \end{equation}[m
[31m-    In particular, for $G(y)=||y||^{\alpha}$ and $G(y)=1$ we get that for $n$ sufficiently large (potentially depending on $x$)[m
[31m-    [m
[31m-    \begin{equation*}[m
[31m-        \int ||y||^{\alpha}k(y)p(x+h_ny)dy\leq c_1 p(x)[m
[31m-    \end{equation*}[m
[31m-    [m
[31m-    and[m
[31m-    [m
[31m-    \begin{equation*}[m
[31m-        \int k(y)p(x+h_ny)dy\geq c_2p(x)[m
[31m-    \end{equation*}[m
[31m-    where $c_1,c_2>0$ depend on $k$ and the Holder constants of $p$.[m
[31m-    Hence, using these estimates in (\ref{eqn:bias}), we get[m
[31m-    [m
[31m-    \begin{equation*}[m
[31m-        |\frac{T_{k_n}(f)(x)}{T_{k_n}(1)(x)}-f(x)|\leq ch_n^{\alpha}[m
[31m-    \end{equation*}[m
[31m-    where $c>0$ is an absolute constant depending on $k$ and the Holder constants of $f$ and $p$.[m
[31m-\end{itemize}[m
[31m-[m
[31m-\paragraph{suboptimal stuff} \paragraph{Lemma 4} Suppose that $p$ is $\beta$-Holder continuous, i.e. there is an $L>0$ such that for all $x,z\in\mathbb{R}^d$[m
[31m-\begin{equation*}[m
[31m-    |p(x)-p(z)|\leq L||x-z||^{\beta}[m
[31m-\end{equation*}[m
[31m-Suppose also that $p(x)>0$.[m
[31m-\begin{itemize}[m
[31m-    \item if $\lambda_nh_n^d=\omega(\frac{1}{\sqrt{n}})$[m
[31m-    then $\hat{f}_{GNW}(x)\rightarrow f(x)$ in probability.[m
[31m-    \item if $\lambda_nh_n^d=\omega(\sqrt{\frac{\log{n}}{n}})$[m
[31m-    then $\hat{f}_{GNW}(x)\rightarrow f(x)$ almost surely.[m
[31m-\end{itemize}[m
[31m-\begin{proof}[m
[31m-We begin by observing that $c_n(x)=T_{k_n}(1)(x)$ is important in the concentration of $\hat{f}_{GNW}$. In order to keep the concentration property we need to have[m
[31m-[m
[31m-[m
[31m-\begin{equation}[m
[31m-\label{eqn:equation_3}[m
[31m-    \lim_{n\rightarrow\infty} c_n(x)^2n=\infty[m
[31m-\end{equation}[m
[31m-We recall the expression[m
[31m-\begin{equation*}[m
[31m-    c_n(x)=\lambda_n\int k(\frac{x-z}{h_n})p(z)dz=\lambda_n h_n^d \int k(y)p(x+h_ny)dy[m
[31m-\end{equation*}[m
[31m-Using the $\beta$-Holder assumption on $p$, we have [m
[31m-\begin{equation*}[m
[31m-\begin{split}[m
[31m-    |\lambda_n^{-1}h_n^{-d}c_n(x)-p(x)\int k(y)dy|&=|\int k(y)[p(x+h_ny)-p(x)]dy|\\[m
[31m-    &\leq \int k(y)|p(x+h_ny)-p(x)|dy\\[m
[31m-    &\leq Lh_n^{\beta}\int ||y||^{\beta}k(y)dy[m
[31m-\end{split}[m
[31m-\end{equation*}[m
[31m-From here it is easy to see that there are $c_1,c_2>0$ such that for $n$ sufficiently large[m
[31m-(potentially depending on $x$), [m
[31m-[m
[31m-\begin{equation}[m
[31m-\label{eqn:degree_bounds}[m
[31m-    c_1p(x)[m
[31m-    \lambda_nh_n^d\leq c_n(x)\leq c_2p(x) \lambda_nh_n^d[m
[31m-\end{equation}[m
[31m-\begin{itemize}[m
[31m-    \item  Suppose that $\lambda_nh_n^d=\omega(\frac{1}{\sqrt{n}})$. Then using Theorem 1 and \ref{eqn:degree_bounds} we get that $\hat{f}_{GNW}(x)\rightarrow f(x)$ as $n\rightarrow\infty$.[m
[31m-    \item Suppose that $\lambda_nh_n^d=\omega(\sqrt{\frac{\log{n}}{n}})$. Then for $n$ sufficiently large,  $\exp(-c_1\lambda_n^2h_n^{2d}n)\leq n^{-(1+r)}$ and hence by Borel-Cantelli lemma, [m
[31m-    \begin{equation*}[m
[31m-        \hat{f}_{GNW}(x)-\frac{T_{k_n}(f)(x)}{T_{k_n}(1)(x)}\rightarrow 0 \textit{ almost surely as } n\rightarrow\infty[m
[31m-    \end{equation*}[m
[31m-   Now the result follows from Lemma 3 (which is a deterministic statement).[m
[31m-\end{itemize}[m
[31m-\end{proof}[m
[31m-[m
[31m-[m
[31m-[m
 \printbibliography[m
 \nocite{*}[m
 [m
[36m@@ -1493,8 +1402,3 @@[m [mFrom here it is easy to see that there are $c_1,c_2>0$ such that for $n$ suffici[m
 [m
 [m
 \end{document}[m
[31m-[m
[31m-%%% Local Variables:[m
[31m-%%% mode: latex[m
[31m-%%% TeX-master: t[m
[31m-%%% End:[m
